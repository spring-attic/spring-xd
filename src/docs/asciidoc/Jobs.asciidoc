
// Empty line above needed after the list from previous file
[[jobs]]
== Jobs

This section describes the job modules included with Spring XD. For a general overview of creating, deploying, and launching batch jobs in Spring XD, see the xref:Batch-Jobs#batch[Batch Jobs] section. To run the examples shown here, start the XD Container
as instructed in the xref:Getting-Started#getting-started[Getting Started] page.

The available jobs are

* <<csv-hdfs, Import CSV Files to HDFS>>
* <<file-jdbc, Import CSV Files to JDBC>>
* <<ftp-hdfs, Import FTP to HDFS>>
* <<gpload, Running gpload as a batch job>>
* <<hdfs-jdbc, Export HDFS to JDBC>>
* <<hdfs-mongodb, Export HDFS to MongoDB>>
* <<jdbc-hdfs, Import JDBC to HDFS>>
* <<spark-app, Running a Spark application as a batch job>>
* <<sqoop, Running Sqoop as a batch job>>

[NOTE]
.Regarding HDFS Configuration
====
To use the hdfs based jobs below, XD needs to have append enabled for hdfs.
Update the hdfs-site.xml with the following settings:

[source,xml]
----
    <property>
        <name>dfs.support.append</name>
        <value>true</value>
    </property>
----
====

[[csv-hdfs]]
=== Import CSV Files to HDFS (`filepollhdfs`)

This module is designed to be driven by a stream polling a directory. It imports data from CSV files and requires that you supply a list of named columns for the data using the `names` parameter. For example:

----
xd:> job create myjob --definition "filepollhdfs --names=forename,surname,address" --deploy
----

You would then use a stream with a file source to scan a directory for files and drive the job. A separate job will be started for each file found:

----
xd:> stream create csvStream --definition "file --mode=ref --dir=/mycsvdir --pattern=*.csv > queue:job:myjob" --deploy

----

//^job.filepollhdfs
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$job.filepollhdfs' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$filepollhdfs$$** $$job$$ has the following options:

$$commitInterval$$:: $$the commit interval to be used for the step$$ *($$int$$, default: `1000`)*
$$deleteFiles$$:: $$whether to delete files after successful import$$ *($$boolean$$, default: `false`)*
$$directory$$:: $$the directory to write the file(s) to in HDFS$$ *($$String$$, default: `/xd/<job name>`)*
$$fileExtension$$:: $$the file extension to use$$ *($$String$$, default: `csv`)*
$$fileName$$:: $$the filename to use in HDFS$$ *($$String$$, default: `<job name>`)*
$$fsUri$$:: $$the URI to use to access the Hadoop FileSystem$$ *($$String$$, default: `${spring.hadoop.fsUri}`)*
$$names$$:: $$the field names in the CSV file$$ *($$String$$, no default)*
$$restartable$$:: $$whether the job should be restartable or not in case of failure$$ *($$boolean$$, default: `false`)*
$$rollover$$:: $$the number of bytes to write before creating a new file in HDFS$$ *($$int$$, default: `1000000`)*
//$job.filepollhdfs

[[file-jdbc]]
=== Import CSV Files to JDBC (`filejdbc`)

A module which loads CSV files into a JDBC table using a single batch job. By default it uses the internal HSQL DB which is used by Spring Batch. Refer to xref:Modules#module_values[how module options are resolved] for further details on how to change defaults (one can of course always use `--foo=bar` notation in the job definition to achieve the same effect).

NOTE: If you access any database other than HSQLDB or Postgres in a job module then the JDBC driver jar for that database needs to be present in the `$XD_HOME/lib` directory.

//^job.filejdbc
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$job.filejdbc' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$filejdbc$$** $$job$$ has the following options:

$$abandonWhenPercentageFull$$:: $$connections that have timed out wont get closed and reported up unless the number of connections in use are above the percentage$$ *($$int$$, default: `0`)*
$$alternateUsernameAllowed$$:: $$uses an alternate user name if connection fails$$ *($$boolean$$, default: `false`)*
$$commitInterval$$:: $$the commit interval to be used for the step$$ *($$int$$, default: `1000`)*
$$connectionProperties$$:: $$connection properties that will be sent to our JDBC driver when establishing new connections$$ *($$String$$, no default)*
$$deleteFiles$$:: $$whether to delete files after successful import$$ *($$boolean$$, default: `false`)*
$$delimiter$$:: $$the delimiter for the delimited file$$ *($$String$$, default: `,`)*
$$driverClassName$$:: $$the JDBC driver to use$$ *($$String$$, no default)*
$$fairQueue$$:: $$set to true if you wish that calls to getConnection should be treated fairly in a true FIFO fashion$$ *($$boolean$$, default: `true`)*
$$fsUri$$:: $$the URI to use to access the Hadoop FileSystem$$ *($$String$$, default: `${spring.hadoop.fsUri}`)*
$$initSQL$$:: $$custom query to be run when a connection is first created$$ *($$String$$, no default)*
$$initialSize$$:: $$initial number of connections that are created when the pool is started$$ *($$int$$, default: `0`)*
$$initializeDatabase$$:: $$whether the database initialization script should be run$$ *($$boolean$$, default: `false`)*
$$initializerScript$$:: $$the name of the SQL script (in /config) to run if 'initializeDatabase' is set$$ *($$String$$, default: `init_batch_import.sql`)*
$$jdbcInterceptors$$:: $$semicolon separated list of classnames extending org.apache.tomcat.jdbc.pool.JdbcInterceptor$$ *($$String$$, no default)*
$$jmxEnabled$$:: $$register the pool with JMX or not$$ *($$boolean$$, default: `true`)*
$$logAbandoned$$:: $$flag to log stack traces for application code which abandoned a Connection$$ *($$boolean$$, default: `false`)*
$$maxActive$$:: $$maximum number of active connections that can be allocated from this pool at the same time$$ *($$int$$, default: `100`)*
$$maxAge$$:: $$time in milliseconds to keep this connection$$ *($$int$$, default: `0`)*
$$maxIdle$$:: $$maximum number of connections that should be kept in the pool at all times$$ *($$int$$, default: `100`)*
$$maxWait$$:: $$maximum number of milliseconds that the pool will wait for a connection$$ *($$int$$, default: `30000`)*
$$minEvictableIdleTimeMillis$$:: $$minimum amount of time an object may sit idle in the pool before it is eligible for eviction$$ *($$int$$, default: `60000`)*
$$minIdle$$:: $$minimum number of established connections that should be kept in the pool at all times$$ *($$int$$, default: `10`)*
$$names$$:: $$the field names in the CSV file$$ *($$String$$, no default)*
$$partitionResultsTimeout$$:: $$time (ms) that the partition handler will wait for results$$ *($$long$$, default: `3600000`)*
$$password$$:: $$the JDBC password$$ *($$Password$$, no default)*
$$removeAbandoned$$:: $$flag to remove abandoned connections if they exceed the removeAbandonedTimout$$ *($$boolean$$, default: `false`)*
$$removeAbandonedTimeout$$:: $$timeout in seconds before an abandoned connection can be removed$$ *($$int$$, default: `60`)*
$$resources$$:: $$the list of paths to import (Spring resources)$$ *($$String$$, no default)*
$$restartable$$:: $$whether the job should be restartable or not in case of failure$$ *($$boolean$$, default: `false`)*
$$suspectTimeout$$:: $$this simply logs the warning after timeout, connection remains$$ *($$int$$, default: `0`)*
$$tableName$$:: $$the database table to which the data will be written$$ *($$String$$, default: `<job name>`)*
$$testOnBorrow$$:: $$indication of whether objects will be validated before being borrowed from the pool$$ *($$boolean$$, default: `false`)*
$$testOnReturn$$:: $$indication of whether objects will be validated before being returned to the pool$$ *($$boolean$$, default: `false`)*
$$testWhileIdle$$:: $$indication of whether objects will be validated by the idle object evictor$$ *($$boolean$$, default: `false`)*
$$timeBetweenEvictionRunsMillis$$:: $$number of milliseconds to sleep between runs of the idle connection validation/cleaner thread$$ *($$int$$, default: `5000`)*
$$url$$:: $$the JDBC URL for the database$$ *($$String$$, no default)*
$$useEquals$$:: $$true if you wish the ProxyConnection class to use String.equals$$ *($$boolean$$, default: `true`)*
$$username$$:: $$the JDBC username$$ *($$String$$, no default)*
$$validationInterval$$:: $$avoid excess validation, only run validation at most at this frequency - time in milliseconds$$ *($$long$$, default: `30000`)*
$$validationQuery$$:: $$sql query that will be used to validate connections from this pool$$ *($$String$$, no default)*
$$validatorClassName$$:: $$name of a class which implements the org.apache.tomcat.jdbc.pool.Validator$$ *($$String$$, no default)*
//$job.filejdbc

The job should be defined with the `resources` parameter defining the files which should be loaded. It also requires a `names` parameter (for the CSV field names) and these should match the database column names into which the data should be stored. You can either pre-create the database table or the module will create it for you if you use `--initializeDatabase=true` when the job is created. The table initialization is configured in a similar way to the JDBC sink and uses the same parameters. The default table name is the job name and can be customized by setting the `tableName` parameter. As an example, if you run the command

----
xd:> job create myjob --definition "filejdbc --resources=file:///mycsvdir/*.csv --names=forename,surname,address --tableName=people --initializeDatabase=true" --deploy
----

it will create the table "people" in the database with three varchar columns called "forename", "surname" and "address". When you launch the job it will load the files matching the resources pattern and write the data to this table. As with the `filepollhdfs` job, this module also supports the `deleteFiles` parameter which will remove the files defined by the `resources` parameter on successful completion of the job.

Launch the job using:

----
xd:> job launch myjob
----

TIP: The connection pool settings for xd are located in servers.yml (i.e. `spring.datasource.*` )

[[ftp-hdfs]]
=== Import FTP to HDFS (`ftphdfs`)

Copies files from FTP directory into HDFS. Job is partitioned in a way that each
separate file copy is executed on its own partitioned step.

An example which copies files:
----
job create --name ftphdfsjob --definition "ftphdfs --host=ftp.example.com --port=21" --deploy
job launch --name ftphdfsjob --params {"remoteDirectory":"/pub/files","hdfsDirectory":"/ftp"}
----

Full path is preserved so that above command would result files in HDFS shown below:
----
/ftp/pub/files
/ftp/pub/files/file1.txt
/ftp/pub/files/file2.txt
----

//^job.ftphdfs
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$job.ftphdfs' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$ftphdfs$$** $$job$$ has the following options:

$$fsUri$$:: $$the URI to use to access the Hadoop FileSystem$$ *($$String$$, default: `${spring.hadoop.fsUri}`)*
$$host$$:: $$the host name for the FTP server$$ *($$String$$, default: `localhost`)*
$$partitionResultsTimeout$$:: $$time (ms) that the partition handler will wait for results$$ *($$long$$, default: `3600000`)*
$$password$$:: $$the password for the FTP connection$$ *($$Password$$, no default)*
$$port$$:: $$the port for the FTP server$$ *($$int$$, default: `21`)*
$$restartable$$:: $$whether the job should be restartable or not in case of failure$$ *($$boolean$$, default: `false`)*
$$username$$:: $$the username for the FTP connection$$ *($$String$$, no default)*
//$job.ftphdfs


[[gpload]]
=== Running gpload as a batch job (`gpload`)
The gpload utility can be deployed and launched from Spring XD as a batch job. The gpload job uses a `GploadTasklet` that submits a gpload job as an external process. The Spring XD gpload batch job aims to support most of the gpload functionality.

We need to provide the following required options:

- `gploadHome` - this must be the path to where gpload utility is installed. This is usually /usr/local/greenplum-loaders-<version>.
- `controlFile` - this file defines the gpload options in effect for this load job and is documented in the _Greenplum Load Tools Reference_ documentation.
- `password` or `passswordFile` - you can either speciy the passord or provide a password file that must follow the general format for a PostgreSQL password file.

Here is an example of a basic load job definition. Please note that some options like host, port, database and username could have been specified in the control file as well.

The content of the control file:
----
VERSION: 1.0.0.1
GPLOAD:
   INPUT:
    - SOURCE:
        FILE: [/home/demo/data/test_file.csv]
    - FORMAT: CSV
    - DELIMITER: ','
    - NULL_AS: '\N'
    - QUOTE: '"'
    - HEADER: FALSE
    - ENCODING: 'UTF8'
    - ERROR_LIMIT: 1000
    - ERROR_TABLE: public.err_table
   OUTPUT:
    - TABLE: demo.test
    - MODE: INSERT
   PRELOAD:
    - TRUNCATE: FALSE
    - REUSE_TABLES: FALSE
----

This is the command used to create and launch the job:

----
xd:>job create myload --definition "gpload --gploadHome=/usr/local/greenplum-loaders-4.3.4.1-build-2 --controlFile=/home/demo/basic.yml --host=pivhdsne --port=5432 --database=pivotal --username=gpadmin --passwordFile=/home/demo/.pgpass" --deploy
xd:>job launch --name myload
----

Once the job is launched, go to Spring XD admin-ui to verify the job results.
Jobs → Executions → Select the job to verify that step execution context holds the log for gpload execution results.

We can override the file name for the source file by providing it as a job parameter like this:

----
job launch --name myload --params {"input.source.file":"/home/demo/data/inputfile2.csv"}
----

This allows us to define a stream to capture new files created in a specific directory:

----
xd>stream create loadFiles --definition "file --ref=true --dir=/home/demo/input --pattern='*.csv' | transform --expression='{\"input.source.file\":\"'+#{'payload.getAbsolutePath()'}+'\"}' > queue:job:myload" --deploy
----

Now, any new file created in that directory will launch a gpload job for that new file.

//^job.gpload
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$job.gpload' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$gpload$$** $$job$$ has the following options:

$$controlFile$$:: $$path to the gpload control file$$ *($$String$$, no default)*
$$database$$:: $$the name of the database to load into$$ *($$String$$, no default)*
$$gploadHome$$:: $$the gpload home location$$ *($$String$$, no default)*
$$host$$:: $$the host name for the Greenplum master database server$$ *($$String$$, no default)*
$$options$$:: $$the gpload options to use$$ *($$String$$, no default)*
$$password$$:: $$the password to use when connecting$$ *($$String$$, no default)*
$$passwordFile$$:: $$the location of the password file$$ *($$String$$, no default)*
$$port$$:: $$the port for the Greenplum master database server$$ *($$Integer$$, no default)*
$$username$$:: $$the username to connect as$$ *($$String$$, no default)*
//$job.gpload

[[hdfs-jdbc]]
=== Export HDFS to JDBC (`hdfsjdbc`)

This module functions very similarly to the `filejdbc` one except that the resources you specify should actually be in HDFS, rather than the OS filesystem.

----
xd:> job create myjob --definition "hdfsjdbc --resources=/xd/data/*.csv --names=forename,surname,address --tableName=people --initializeDatabase=true" --deploy
----

Launch the job using:

----
xd:> job launch myjob
----

NOTE: If you access any database other than HSQLDB or Postgres in a job module then the JDBC driver jar for that database needs to be present in the `$XD_HOME/lib` directory.

//^job.hdfsjdbc
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$job.hdfsjdbc' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$hdfsjdbc$$** $$job$$ has the following options:

$$abandonWhenPercentageFull$$:: $$connections that have timed out wont get closed and reported up unless the number of connections in use are above the percentage$$ *($$int$$, default: `0`)*
$$alternateUsernameAllowed$$:: $$uses an alternate user name if connection fails$$ *($$boolean$$, default: `false`)*
$$commitInterval$$:: $$the commit interval to be used for the step$$ *($$int$$, default: `1000`)*
$$connectionProperties$$:: $$connection properties that will be sent to our JDBC driver when establishing new connections$$ *($$String$$, no default)*
$$delimiter$$:: $$the delimiter for the delimited file$$ *($$String$$, default: `,`)*
$$driverClassName$$:: $$the JDBC driver to use$$ *($$String$$, no default)*
$$fairQueue$$:: $$set to true if you wish that calls to getConnection should be treated fairly in a true FIFO fashion$$ *($$boolean$$, default: `true`)*
$$fsUri$$:: $$the URI to use to access the Hadoop FileSystem$$ *($$String$$, default: `${spring.hadoop.fsUri}`)*
$$initSQL$$:: $$custom query to be run when a connection is first created$$ *($$String$$, no default)*
$$initialSize$$:: $$initial number of connections that are created when the pool is started$$ *($$int$$, default: `0`)*
$$initializeDatabase$$:: $$whether the database initialization script should be run$$ *($$boolean$$, default: `false`)*
$$initializerScript$$:: $$the name of the SQL script (in /config) to run if 'initializeDatabase' is set$$ *($$String$$, default: `init_batch_import.sql`)*
$$jdbcInterceptors$$:: $$semicolon separated list of classnames extending org.apache.tomcat.jdbc.pool.JdbcInterceptor$$ *($$String$$, no default)*
$$jmxEnabled$$:: $$register the pool with JMX or not$$ *($$boolean$$, default: `true`)*
$$logAbandoned$$:: $$flag to log stack traces for application code which abandoned a Connection$$ *($$boolean$$, default: `false`)*
$$maxActive$$:: $$maximum number of active connections that can be allocated from this pool at the same time$$ *($$int$$, default: `100`)*
$$maxAge$$:: $$time in milliseconds to keep this connection$$ *($$int$$, default: `0`)*
$$maxIdle$$:: $$maximum number of connections that should be kept in the pool at all times$$ *($$int$$, default: `100`)*
$$maxWait$$:: $$maximum number of milliseconds that the pool will wait for a connection$$ *($$int$$, default: `30000`)*
$$minEvictableIdleTimeMillis$$:: $$minimum amount of time an object may sit idle in the pool before it is eligible for eviction$$ *($$int$$, default: `60000`)*
$$minIdle$$:: $$minimum number of established connections that should be kept in the pool at all times$$ *($$int$$, default: `10`)*
$$names$$:: $$the field names in the CSV file$$ *($$String$$, no default)*
$$password$$:: $$the JDBC password$$ *($$Password$$, no default)*
$$removeAbandoned$$:: $$flag to remove abandoned connections if they exceed the removeAbandonedTimout$$ *($$boolean$$, default: `false`)*
$$removeAbandonedTimeout$$:: $$timeout in seconds before an abandoned connection can be removed$$ *($$int$$, default: `60`)*
$$resources$$:: $$the list of paths to import (Spring resources)$$ *($$String$$, no default)*
$$restartable$$:: $$whether the job should be restartable or not in case of failure$$ *($$boolean$$, default: `false`)*
$$suspectTimeout$$:: $$this simply logs the warning after timeout, connection remains$$ *($$int$$, default: `0`)*
$$tableName$$:: $$the database table to which the data will be written$$ *($$String$$, default: `<job name>`)*
$$testOnBorrow$$:: $$indication of whether objects will be validated before being borrowed from the pool$$ *($$boolean$$, default: `false`)*
$$testOnReturn$$:: $$indication of whether objects will be validated before being returned to the pool$$ *($$boolean$$, default: `false`)*
$$testWhileIdle$$:: $$indication of whether objects will be validated by the idle object evictor$$ *($$boolean$$, default: `false`)*
$$timeBetweenEvictionRunsMillis$$:: $$number of milliseconds to sleep between runs of the idle connection validation/cleaner thread$$ *($$int$$, default: `5000`)*
$$url$$:: $$the JDBC URL for the database$$ *($$String$$, no default)*
$$useEquals$$:: $$true if you wish the ProxyConnection class to use String.equals$$ *($$boolean$$, default: `true`)*
$$username$$:: $$the JDBC username$$ *($$String$$, no default)*
$$validationInterval$$:: $$avoid excess validation, only run validation at most at this frequency - time in milliseconds$$ *($$long$$, default: `30000`)*
$$validationQuery$$:: $$sql query that will be used to validate connections from this pool$$ *($$String$$, no default)*
$$validatorClassName$$:: $$name of a class which implements the org.apache.tomcat.jdbc.pool.Validator$$ *($$String$$, no default)*
//$job.hdfsjdbc

TIP: The connection pool settings for xd are located in servers.yml (i.e. `spring.datasource.*` )

[[hdfs-mongodb]]
=== Export HDFS to MongoDB (`hdfsmongodb`)

Exports CSV data from HDFS and stores it in a MongoDB collection which defaults to the job name. This can be overridden with the `collectionName` parameter. Once again, the field names should be defined by supplying the `names` parameter. The data is converted internally to a Spring XD `Tuple` and the collection items will have an `id` matching the tuple's UUID. You can override this by setting the `idField` parameter to one of the field names if desired.

An example:

----
xd:> job create myjob --definition "hdfsmongodb --resources=/data/*.log --names=employeeId,forename,surname,address --idField=employeeId --collectionName=people" --deploy
----

//^job.hdfsmongodb
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$job.hdfsmongodb' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$hdfsmongodb$$** $$job$$ has the following options:

$$authenticationDatabaseName$$:: $$the MongoDB authentication database used for connecting$$ *($$String$$, default: ``)*
$$collectionName$$:: $$the MongoDB collection to store$$ *($$String$$, default: `<job name>`)*
$$commitInterval$$:: $$the commit interval to be used for the step$$ *($$int$$, default: `1000`)*
$$databaseName$$:: $$the MongoDB database name$$ *($$String$$, default: `xd`)*
$$delimiter$$:: $$the delimiter for the delimited file$$ *($$String$$, default: `,`)*
$$fsUri$$:: $$the URI to use to access the Hadoop FileSystem$$ *($$String$$, default: `${spring.hadoop.fsUri}`)*
$$host$$:: $$the MongoDB host to connect to$$ *($$String$$, default: `localhost`)*
$$idField$$:: $$the name of the field to use as the identity in MongoDB$$ *($$String$$, no default)*
$$names$$:: $$the field names in the CSV file$$ *($$String$$, no default)*
$$password$$:: $$the MongoDB password used for connecting$$ *($$String$$, default: ``)*
$$port$$:: $$the MongoDB port to connect to$$ *($$int$$, default: `27017`)*
$$resources$$:: $$the list of paths to import (Spring resources)$$ *($$String$$, no default)*
$$restartable$$:: $$whether the job should be restartable or not in case of failure$$ *($$boolean$$, default: `false`)*
$$username$$:: $$the MongoDB username used for connecting$$ *($$String$$, default: ``)*
$$writeConcern$$:: $$the default MongoDB write concern to use$$ *($$WriteConcern$$, default: `SAFE`, possible values: `NONE,NORMAL,SAFE,FSYNC_SAFE,REPLICAS_SAFE,JOURNAL_SAFE,MAJORITY`)*
//$job.hdfsmongodb

[[jdbc-hdfs]]
=== Import JDBC to HDFS (`jdbchdfs`)

Performs the reverse of the previous module. The database configuration is the same as for `filejdbc` but without the initialization options since you need to already have the data to import into HDFS. When creating the job, you must either supply the select statement by setting the `sql` parameter, or you can supply both `tableName` and `columns` options (which will be used to build the SQL statement).


To import data from the database table `some_table`, you could use

----
xd:> job create myjob --definition "jdbchdfs --sql='select col1,col2,col3 from some_table'" --deploy
----

You can customize how the data is written to HDFS by supplying the options `directory` (defaults to `/xd/(job name)`), `fileName` (defaults to job name), `rollover` (in bytes, default 1000000) and `fileExtension` (defaults to 'csv').

Launch the job using:

----
xd:> job launch myjob
----

NOTE: If you access any database other than HSQLDB or Postgres in a job module then the JDBC driver jar for that database needs to be present in the `$XD_HOME/lib` directory.

If you want to partition your job across multiple XD containers you can provide the `partitionColumn` and `partitions` option. When the job is launched the partitioner will query the database for the range of values and evenly divide the load between the partitions. This assumes that there is an even distribution of column values in the table. When using the partitioning support you must also use the `tableName` and `columns` options instead of the `sql` option. This is so the partitioner can construct the queries with the appropriate where clauses for the different partitions.

An example of a partitioned job could look like this:

----
xd:> job create partitionedJob --definition "jdbchdfs --columns='id,col1,col2' --tableName=some_table --partitionColumn=id --partitions=4" --deploy
----

NOTE: When using the partitioning support you can not use the `sql` option. Use `tableName` and `columns` instead.

You can perform incremental imports using this job by defining a column to check against.  Currently the column must be numeric (similar to how the partitionColumn works).  An example of launching a job that performs incremental imports would look like the following:

----
xd:> job create incrementalImportJob --definition "jdbchdfs --columns='id,col1,col2' --tableName=some_table --checkColumn=sequence --restartable=true" --deploy
----

If you want to specify the value for the `checkColumn`, you can pass the override value in as a `JobParameter` named `overrideCheckColumnValue` as shown below:

----
xd:> job launch incrementalImportJob --params {"overrideCheckColumnValue" : 2}
----

There are two things to keep in mind when using incremental imports with this job:

* When using incremental imports, the `sql` option is not available.  Use `tableName` and `columns` instead.
* If an import fails, it must be rerun to completion before running the next import.  Without this, inconsistent data may result.  Since HDFS is a non-transactional store, failed records may not be rolled back.  An administrator may need to check HDFS for completeness and the last imported value.


//^job.jdbchdfs
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$job.jdbchdfs' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$jdbchdfs$$** $$job$$ has the following options:

$$abandonWhenPercentageFull$$:: $$connections that have timed out wont get closed and reported up unless the number of connections in use are above the percentage$$ *($$int$$, default: `0`)*
$$alternateUsernameAllowed$$:: $$uses an alternate user name if connection fails$$ *($$boolean$$, default: `false`)*
$$checkColumn$$:: $$the column to be examined when determining which rows to import$$ *($$String$$, default: ``)*
$$columns$$:: $$the column names to read from the supplied table$$ *($$String$$, default: ``)*
$$commitInterval$$:: $$the commit interval to be used for the step$$ *($$int$$, default: `1000`)*
$$connectionProperties$$:: $$connection properties that will be sent to our JDBC driver when establishing new connections$$ *($$String$$, no default)*
$$delimiter$$:: $$the delimiter for the delimited file$$ *($$String$$, default: `,`)*
$$directory$$:: $$the directory to write the file(s) to in HDFS$$ *($$String$$, default: `/xd/<job name>`)*
$$driverClassName$$:: $$the JDBC driver to use$$ *($$String$$, no default)*
$$fairQueue$$:: $$set to true if you wish that calls to getConnection should be treated fairly in a true FIFO fashion$$ *($$boolean$$, default: `true`)*
$$fileExtension$$:: $$the file extension to use$$ *($$String$$, default: `csv`)*
$$fileName$$:: $$the filename to use in HDFS$$ *($$String$$, default: `<job name>`)*
$$fsUri$$:: $$the URI to use to access the Hadoop FileSystem$$ *($$String$$, default: `${spring.hadoop.fsUri}`)*
$$initSQL$$:: $$custom query to be run when a connection is first created$$ *($$String$$, no default)*
$$initialSize$$:: $$initial number of connections that are created when the pool is started$$ *($$int$$, default: `0`)*
$$jdbcInterceptors$$:: $$semicolon separated list of classnames extending org.apache.tomcat.jdbc.pool.JdbcInterceptor$$ *($$String$$, no default)*
$$jmxEnabled$$:: $$register the pool with JMX or not$$ *($$boolean$$, default: `true`)*
$$logAbandoned$$:: $$flag to log stack traces for application code which abandoned a Connection$$ *($$boolean$$, default: `false`)*
$$maxActive$$:: $$maximum number of active connections that can be allocated from this pool at the same time$$ *($$int$$, default: `100`)*
$$maxAge$$:: $$time in milliseconds to keep this connection$$ *($$int$$, default: `0`)*
$$maxIdle$$:: $$maximum number of connections that should be kept in the pool at all times$$ *($$int$$, default: `100`)*
$$maxWait$$:: $$maximum number of milliseconds that the pool will wait for a connection$$ *($$int$$, default: `30000`)*
$$minEvictableIdleTimeMillis$$:: $$minimum amount of time an object may sit idle in the pool before it is eligible for eviction$$ *($$int$$, default: `60000`)*
$$minIdle$$:: $$minimum number of established connections that should be kept in the pool at all times$$ *($$int$$, default: `10`)*
$$partitionColumn$$:: $$the column to use for partitioning, should be numeric and uniformly distributed$$ *($$String$$, default: ``)*
$$partitionResultsTimeout$$:: $$time (ms) that the partition handler will wait for results$$ *($$long$$, default: `3600000`)*
$$partitions$$:: $$the number of partitions$$ *($$int$$, default: `1`)*
$$password$$:: $$the JDBC password$$ *($$Password$$, no default)*
$$removeAbandoned$$:: $$flag to remove abandoned connections if they exceed the removeAbandonedTimout$$ *($$boolean$$, default: `false`)*
$$removeAbandonedTimeout$$:: $$timeout in seconds before an abandoned connection can be removed$$ *($$int$$, default: `60`)*
$$restartable$$:: $$whether the job should be restartable or not in case of failure$$ *($$boolean$$, default: `false`)*
$$rollover$$:: $$the number of bytes to write before creating a new file in HDFS$$ *($$int$$, default: `1000000`)*
$$sql$$:: $$the SQL to use to extract data$$ *($$String$$, default: ``)*
$$suspectTimeout$$:: $$this simply logs the warning after timeout, connection remains$$ *($$int$$, default: `0`)*
$$tableName$$:: $$the table to read data from$$ *($$String$$, default: ``)*
$$testOnBorrow$$:: $$indication of whether objects will be validated before being borrowed from the pool$$ *($$boolean$$, default: `false`)*
$$testOnReturn$$:: $$indication of whether objects will be validated before being returned to the pool$$ *($$boolean$$, default: `false`)*
$$testWhileIdle$$:: $$indication of whether objects will be validated by the idle object evictor$$ *($$boolean$$, default: `false`)*
$$timeBetweenEvictionRunsMillis$$:: $$number of milliseconds to sleep between runs of the idle connection validation/cleaner thread$$ *($$int$$, default: `5000`)*
$$url$$:: $$the JDBC URL for the database$$ *($$String$$, no default)*
$$useEquals$$:: $$true if you wish the ProxyConnection class to use String.equals$$ *($$boolean$$, default: `true`)*
$$username$$:: $$the JDBC username$$ *($$String$$, no default)*
$$validationInterval$$:: $$avoid excess validation, only run validation at most at this frequency - time in milliseconds$$ *($$long$$, default: `30000`)*
$$validationQuery$$:: $$sql query that will be used to validate connections from this pool$$ *($$String$$, no default)*
$$validatorClassName$$:: $$name of a class which implements the org.apache.tomcat.jdbc.pool.Validator$$ *($$String$$, no default)*
//$job.jdbchdfs

TIP: The connection pool settings for xd are located in servers.yml (i.e. `spring.datasource.*` )

[[spark-app]]
=== Running Spark application as a batch job (`sparkapp`)
A Spark Application can be deployed and launched from Spring XD as a batch job. SparkTasklet submits the Spark application into Spark cluster manager using **org.apache.spark.deploy.SparkSubmit**. Through this approach, you can also launch a Spark application with specific criteria via Spring XD stream (for instance: A real time scoring algorithm through MLlib spark job can be triggered based on the streaming data events). To get started, please refer to Spark examples here: https://spark.apache.org/examples.html.

NOTE: The current Spark release that is supported is Spark 1.2.1

Lets run some Spark examples as Spring XD batch jobs:
----
xd:>job create SparkPiExample --definition "sparkapp --appJar=<the location of spark-examples-1.2.1 jar> --name=MyApp --master=<spark master url or local> --mainClass=org.apache.spark.examples.SparkPi" --deploy
xd:>job launch SparkPiExample
----
----
xd:>job create JavaWordCountExample --definition "sparkapp --appJar=<the location of spark-examples-1.2.1 jar> --name=MyApp --master=<spark master url or local> --mainClass=org.apache.spark.examples.JavaWordCount --programArgs=<location of the file to count the words>" --deploy
xd>job launch JavaWordCountExample
----

Once the job is launched, go to Spring XD admin-ui to verify the job results.
Jobs → Executions → Select the job to verify that execution context holds the log for Spark application results. If you launch the Spark application through Spark Master, then the results and application status can be verified from SparkUI as well.

//^job.sparkapp
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$job.sparkapp' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$sparkapp$$** $$job$$ has the following options:

$$appJar$$:: $$path to a bundled jar that includes your application and its dependencies - excluding spark$$ *($$String$$, no default)*
$$conf$$:: $$comma seperated list of key value pairs as config properties$$ *($$String$$, default: ``)*
$$files$$:: $$comma separated list of files to be placed in the working directory of each executor$$ *($$String$$, default: ``)*
$$mainClass$$:: $$the main class for Spark application$$ *($$String$$, no default)*
$$master$$:: $$the master URL for Spark$$ *($$String$$, default: `local`)*
$$name$$:: $$the name of the Spark application$$ *($$String$$, default: ``)*
$$programArgs$$:: $$program arguments for the application main class$$ *($$String$$, default: ``)*
//$job.sparkapp

[[sqoop]]
=== Running Sqoop as a batch job (`sqoop`)
A Sqoop job can be deployed and launched from Spring XD as a batch job. The Sqoop job uses a `SqoopTasklet` and a `SqoopRunner` that submits a Sqoop job using **org.apache.sqoop.Sqoop.runTool**. The Spring XD Sqoop batch job aims to support most of the Sqoop functionality, but at this point we have only tested a subset:

* import
* export
* codegen
* merge
* job
* list-tables

NOTE: The current release supports Sqoop 1.4.5

The intention is to eventually support all features of the Sqoop tool. See https://sqoop.apache.org/docs/1.4.5/SqoopUserGuide.html[Sqoop User Guide] for full documentation of the Sqoop features.

NOTE: If you access any database other than HSQLDB or Postgres in a job module then the JDBC driver jar for that database needs to be present in the `$XD_HOME/lib` directory.

We can test the Sqoop job by just listing the tables in the database:

----
xd:>job create sqoopListTables --definition "sqoop --command=list-tables" --deploy
xd:>job launch --name sqoopListTables
----

The definition contains the name of the provided job as `sqoop` and the `--command` option names the Sqoop command we want to run, which in this case is "list-tables".

Once the job is launched, go to Spring XD admin-ui to verify the job results.
Jobs → Executions → Select the job to verify that step execution context holds the log for Sqoop Tool execution results. You should see some tables listed there. Since we didn't provide any connection arguments Spring XD will by default use the batch respoitory database for the Sqoop Tool execution. We could provide options specifying a different database using the `--url`, `--username` and `--password` options for the job:

----
xd:>job create sqoopListTables2 --definition "sqoop --command=list-tables --url=jdbc:mysql://localhost:3306/test --username=myuser --password=mypasswd" --deploy
xd:>job launch --name sqoopListTables2
----

Here we connect to a local MySQL database. It's important to note that you need to provide the MySQL JDBC driver jar in the Spring XD lib directory for this to work.

There also is an option to specify connection arguments using the `--args` option. This allows you to use the same arguments that you are used to provide on the command line when running the Sqoop Tool directly. To connect to the same MySQL database as above using `--args` we would use:

----
xd:>job create sqoopListTables3 --definition "sqoop --command=list-tables --args='--connect=jdbc:mysql://localhost:3306/test --username=myuser --password=mypasswd'" --deploy
xd:>job launch --name sqoopListTables3
----

When importing data, you simply use "import" as the command to run. Here is an example:

----
xd:>job create sqoopImport1 --definition "sqoop --command=import --args='--table=MYTABLE' --url=jdbc:mysql://localhost:3306/test --username=myuser --password=mypasswd" --deploy
xd:>job launch --name sqoopImport1
----

In this example we provided the connection arguments using the `-args` option. We could also have used `--url`, `--username` and `--password` options like we did above for the "list-tables" example. The "import" command will use the `spring.hadoop.fsUri` that is specified when Spring XD starts up. You can override this by providing the `--fsUri` option when defining the job. The same is true for `spring.hadoop.resourceManagerHost` and `spring.hadoop.resourceManagerPort`. You can override the Spring XD configured values with `--resourceManagerHost` and `--resourceManagerPort` options.

For exports we use the "export" command. Here is an example:

----
xd:>job create sqoopExport1 --definition "sqoop --command=export --args='--table=NEWTABLE --export-dir=/user/xduser/MYTABLE'" --deploy
xd:>job launch --name sqoopExport1
----

Here we rely on the connection options to default to the same database used for the batch repository. Note that Sqoop requires that the table to export data into must already exist.

NOTE: If your Sqoop args are more complex, as is the case when you provide a query expression or a where clause, then you will need to use escaping for double quotes used within the `--args` option. A quick example of using a where clause:

----
job create sqoopComplexArgs1 --definition "sqoop --command=import --args='--table MYFILES --where \"ID < 390000\" --target-dir /user/xduser/TEST --split-by ID'"
----

(For this example we have omitted the equal sign for the individual Sqoop arguments within the `--args` option. Either style works fine.)

NOTE: If your Sqoop args use escape sequences (common when working with Hive data) then you should provide double back-slash characters when working with the XD Shell (this effectively escapes the escape character and only one back-slash will be passed on). Here is a brief example:

----
job create sqoopHiveArgs1 --definition "sqoop --command=import --args='--table MYFILES --target-dir /user/xduser/TEST --split-by ID --null-string \\\\N --fields-terminated-by \\0001'"
----

For more detailed coverage of using quotes and escaping please see xref:DSL-Reference#dsl-quotes-escaping[Single quotes, Double quotes, Escaping].

NOTE: Some JDBC drivers and some compression codecs require additional jars to work properly. If this is the case, then you can use the `--libjars` option to provide a comma separated list of jars to be added to the job execution. You should only specify the name of the jar and not the full path. The jars will be looked up from the classpath and included in the job submitted to the Hadoop cluster.

NOTE: When using Sqoop's `--as-avrodatafile` argument we will automatically include the Avro jars in the Sqoop job submission. No need to specify them as part of the `--libjars` option.

NOTE: Advanced Hadoop configuration options can be provided in one of several configuration files. The `hadoop-site.xml` file is only used by the Sqoop job while the other configuration files are used by all Hadoop related jobs and streams:

- `$XD_HOME/config/hadoop.properties` -- just add the property you would like to set:
+
----
dfs.client.socket-timeout=20000
----
- `$XD_HOME/config/hadoop-site.xml` -- add a property entry:
+
[source,xml]
----
    <property>
      <name>dfs.client.socket-timeout</name>
      <value>20000</value>
    </property>
----
- `$XD_HOME/config/servers.yml` -- add a spring.hadoop.config entry:
+
[source,yml]
----
spring:
  hadoop:
    config:
      dfs.client.socket-timeout: 20000
----

==== Using Sqoop's metastore

It is possible to use Sqoop's metastore with some restrictions.

WARNING: Sqoop ships with HSQLDB version 1.8 and Spring XD ships with HSQLDB version 2.3. Since these two versions are not compatible you can not use a Sqoop metastore
that uses HSQLDB. This is unfortunate since HSQLDB version 1.8 is the only database that is fully supported for the metastore by Sqoop. We can however use another database
for the metastore as long as we use some workarounds.

NOTE: You can use PostgreSQL for the Sqoop metastore. We recommend that you run the commands listed below to create and initialize the tables to be used by the Sqoop metastore.

Create and initialize the Sqoop metastore tables:

----
CREATE TABLE
    SQOOP_ROOT
    (
        version INTEGER,
        propname CHARACTER VARYING(128) NOT NULL,
        propval CHARACTER VARYING(256),
        UNIQUE (version, propname)
    );
CREATE TABLE
    SQOOP_SESSIONS
    (
        job_name CHARACTER VARYING(64) NOT NULL,
        propname CHARACTER VARYING(128) NOT NULL,
        propval CHARACTER VARYING(1024),
        propclass CHARACTER VARYING(32) NOT NULL,
        UNIQUE (job_name, propname, propclass)
    );
INSERT INTO sqoop_root (version, propname, propval) VALUES (null, 'sqoop.hsqldb.job.storage.version', '0');
INSERT INTO sqoop_root (version, propname, propval) VALUES (0, 'sqoop.hsqldb.job.info.table', 'SQOOP_SESSIONS');
----

You can now modify the `scoop-site.xml` file in the Spring XD config directory. Add the JDBC URL, username and password to use for connection to the PostgreSQL database
that hosts the Sqoop metastore tables. You need to provide the following properties:

- `sqoop.metastore.client.autoconnect.url`
- `sqoop.metastore.client.autoconnect.username`
- `sqoop.metastore.client.autoconnect.password`

NOTE: In addition to the above configurations you need to use a `--password-file` option when creating the Sqoop job definitions. If you don't then Sqoop will prompt for a password
as Spring XD runs the job. This will cause the job to hang.

Here is an example of defining a Sqoop job using Spring XD's `sqoop` job:

----
xd>job create job1create --definition "sqoop --command=job --args='--create job1 -- import --table PETS --incremental append --check-column ID --last-value 0 --connect jdbc:hsqldb:hsql://localhost:9001/test --username sa --password-file /xd/hsql.password --target-dir /xd/job1 --num-mappers 1'" --deploy
xd>job launch job1create
----

Here is an example of executing the predefined Sqoop job using Spring XD's `sqoop` job:

----
xd>job create job1exec --definition "sqoop --command=job --args='--exec job1'" --deploy
xd>job launch job1exec
----

==== Options for Sqoop job

//^job.sqoop
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$job.sqoop' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$sqoop$$** $$job$$ has the following options:

$$args$$:: $$the arguments for the Sqoop command$$ *($$String$$, default: ``)*
$$command$$:: $$the Sqoop command to run$$ *($$String$$, default: ``)*
$$driverClassName$$:: $$the JDBC driver to use$$ *($$String$$, no default)*
$$fsUri$$:: $$the URI to use to access the Hadoop FileSystem$$ *($$String$$, default: `${spring.hadoop.fsUri}`)*
$$libjars$$:: $$extra jars from the classpath to add to the job$$ *($$String$$, default: ``)*
$$password$$:: $$the JDBC password$$ *($$Password$$, no default)*
$$resourceManagerHost$$:: $$the Host for Hadoop's ResourceManager$$ *($$String$$, default: `${spring.hadoop.resourceManagerHost}`)*
$$resourceManagerPort$$:: $$the Port for Hadoop's ResourceManager$$ *($$String$$, default: `${spring.hadoop.resourceManagerPort}`)*
$$url$$:: $$the JDBC URL for the database$$ *($$String$$, no default)*
$$username$$:: $$the JDBC username$$ *($$String$$, no default)*
//$job.sqoop

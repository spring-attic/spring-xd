[[running-on-YARN]]
=== Running on YARN

==== Introduction
The Spring XD distributed runtime (DIRT) supports distribution of
processing tasks across multiple nodes. See
xref:Running-Distributed-Mode#running-distributed-mode[Running Distributed Mode] for
information on running Spring XD in distributed mode. One option is to
run these nodes on a Hadoop YARN cluster rather than on VMs or
physical servers managed by you.

==== What do you need?
To begin with, you need to have access to a Hadoop cluster running a
version based on Apache Hadoop version 2. This includes
link:http://www.us.apache.org/dist/hadoop/common/hadoop-2.7.1/[Apache
Hadoop 2.7.1],
link:https://pivotal.io/big-data/pivotal-hd[Pivotal HD 2.1 or 3.0],
link:https://hortonworks.com/hdp/[Hortonworks HDP 2.3] and link:https://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html[Cloudera CDH5].

[IMPORTANT]
====
Running YARN on some Ubuntu distributions and Mac OS X has shown to have issues when YARN applications are killed. It seems that the kill command 
doesn't always succesfully kill the corresponding OS process and you end up with application processes still running. See link:https://issues.apache.org/jira/browse/HADOOP-9752[HADOOP-9752] for more details.
====

You need a supported transport, see
xref:Running-Distributed-Mode#running-distributed-mode[Running Distributed Mode] for
installation of Redis or Rabbit MQ. Spring XD on YARN currently uses
Redis as the default data transport.

You also need Zookeeper running. If your Hadoop cluster doesn't have
Zookeeper installed you need to install and run it specifically for
Spring XD. See the
xref:Running-Distributed-Mode#setting-up-zookeeper[Setting up
ZooKeeper] section of the "Running Distributed Mode" chapter.

Lastly, you need an RDBMs to support batch jobs and JDBC operations.


==== Download Spring XD on YARN binaries
In addition to the regular `spring-xd-<version>-dist.zip` files we
also distribute a zip file that includes all you need to deploy on
YARN. The name of this zip file is `spring-xd-<version>-yarn.zip`. You
can download the zip file for the current release from
link:https://repo.spring.io/release/org/springframework/xd/spring-xd/[Spring release repo] or a milestone build from the 
link:https://repo.spring.io/milestone/org/springframework/xd/spring-xd/[Spring milestone repo]. Unzip the downloaded file and you should see a
`spring-xd-<version>-yarn` directory.

==== Configure your deployment
Configuration options are contained in a `config/servers.yml` file in
the Spring XD YARN install directory. You need to configure the hadoop
settings, the transport choice plus redis/rabbit settings, the
zookeeper settings and the JDBC datasource properties.

Depending on the distribution used you might need to change the
`siteYarnAppClasspath` and `siteMapreduceAppClasspath`. We have
provided basic settings for the supported distros, you just need to
uncomment the ones for the distro you use.

These are the settings used for Hadoop 2.7.1:

[source,yaml]
----
spring:
    yarn:
        siteYarnAppClasspath: "$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/share/hadoop/common/*,$HADOOP_COMMON_HOME/share/hadoop/common/lib/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*,$HADOOP_YARN_HOME/share/hadoop/yarn/*,$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*"
        siteMapreduceAppClasspath: "$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*"

----

===== XD options
For Spring XD you need to define how many admin servers and containers
you need using properties `spring.xd.adminServers` and `spring.xd.containers`
respectively. You also need to define the HDFS location using property
`spring.yarn.applicationDir` where the Spring XD binary and config
files will be stored.

[source,yaml]
----
spring:
  xd:
    appmasterMemory: 512M
    adminServers: 1
    adminMemory: 512M
    adminLocality: false
    containers: 3
    containerMemory: 512M
    containerLocality: false
    container:
      groups: yarn
  yarn:
    applicationDir: /xd/app/
----

More about memory settings in above configuration, see section
<<configuring-yarn-memory-reservations>>.

===== Hadoop settings
You need to specify the host where the YARN Resource Manager is
running using `spring.hadoop.resourceManagerHost` as well as the HDFS
URL using `spring.hadoop.fsUri`.

[source,yaml]
----
# Hadoop properties
spring:
  hadoop:
    fsUri: hdfs://localhost:8020
    resourceManagerHost: localhost
    config:
      topology.script.file.name: /path/to/topology-script.sh
----

[NOTE]
====
Setting hadoop `topology.script.file.name` property is mandatory if
more sophisticated container placement is used to allocate XD admins
or containers from a spesific hosts or racks. If this property is not
set to match a one used in a hadoop cluster, allocations using hosts
and racks will simply fail.
====

===== Zookeeper settings
You should specify the Zookeeper connection settings 

[source,yaml]
----
#Zookeeper properties
#client connect string: host1:port1,host2:port2,...,hostN:portN
zk:
  client:
     connect: localhost:2181
----

===== Transport options
You should choose either `redis` (default) or `rabbit` as the transport
and include the host and port in the properties for the choice you made.

[source,yaml]
----
# Transport used
transport: redis
----

[source,yaml]
----
# Redis properties
spring:
  redis:
   port: 6379
   host: localhost
----

===== JDBC datasource properties
You should specify the JDBC connection properties based on the RDBMs
that you use for the batch jobs and JDBC sink

[source,yaml]
----
#Config for use with MySQL - uncomment and edit with relevant values for your environment
spring:
  datasource:
    url: jdbc:mysql://yourDBhost:3306/yourDB
    username: yourUsername
    password: yourPassword
    driverClassName: com.mysql.jdbc.Driver
----

===== XD Admin port

On default the property `server.port` which defines the used port for
embedded server is set to `9393` but it can be overridden by changing the value in `servers.yml`.

[source,yaml]
----
#Port that admin-ui is listening on
#server:
#  port: 9393
----

On YARN it is recommended that you simply set the port to `0` meaning
that server will automatically choose a random port. This is advisable
simply because it will prevent port collission which are usually a
little difficult to track down from a cluster. See more instructions
in the section <<connect-xd-shell-to-yarn-runtime-managed-admins>> for how to connect
xd-shell to admins managed by YARN.

[source,yaml]
----
#Port that admin-ui is listening on
server:
  port: 0
----

==== Adding custom modules

The recommended approach for custom modules is to define the module registry location as a directory
in HDFS. This will allow the most flexibility and the modules will automatically be available to all XD containers 
running in the Hadoop cluster. The `xd.customModule.home` property is by default set to the value 
`${spring.hadoop.fsUri}/xd/yarn/custom-modules` for YARN deployments. This can be modified, but we recommend keeping 
it to a location on HDFS within the same Hadoop cluster. 

[source,yaml]
----
xd:
  customModule:
    home: ${spring.hadoop.fsUri}/xd/yarn/custom-modules
----

See the xref:Modules#modules[Modules] section for more on 
custom modules.

==== Customizing module configurations

The configurations for all standard XD modules can be customized by modifying the
file `modules.yml` in the `config` directory and then adding it to the `modules-config.zip`
archive in the same directory.

You can run the following command from the `config` directory to
achieve this:

----
jar -uf modules-config.zip modules.yml
----

==== Modify container logging

Logging configuration for XD admins and containers are defined in
files `config/xd-admin-logger.properties` and
`config/xd-container-logger.properties` respectively. These two files
are copied over to hdfs during the deployment. If you want to modify
logging configuration either modify source files and do a deployment
again or modify files in hdfs directly.

==== Control XD YARN application lifecycle

Change current directory to be the directory that was unzipped
`spring-xd-<version>-yarn`. To read about runtime configuration and more
sophisticated features see section
<<working-with-container-groups>>.

===== Push the Spring XD application binaries and config to HDFS

Run the command

[source,bash]
----
$ bin/xd-yarn push
New version installed
----

===== List installed application versions

Run the command

[source,bash]
----
$ bin/xd-yarn pushed
  NAME  PATH
  ----  --------------------
  app   hdfs://node1:8020/xd
----

===== Submit the Spring XD YARN application

Run the command

[source,bash]
----
$ bin/xd-yarn submit
New instance submitted with id application_1420911708637_0001
----


===== Check the status of YARN apps

You can use the regular `yarn` command to check the status. Simply run:

[source,bash]
----
$ bin/xd-yarn submitted
  APPLICATION ID                  USER          NAME    QUEUE    TYPE STARTTIME       FINISHTIME  STATE    FINALSTATUS  ORIGINAL TRACKING URL
  ------------------------------  ------------  ------  ------- ----  --------------  ----------  -------  ----------- ---------------------------
  application_1420911708637_0001  jvalkealahti  xd-app  default XD    09/01/15 14:25  N/A         RUNNING  UNDEFINED https://172.16.101.106:49792
----

You should see one application running named `xd-app`.

[NOTE]
====
Pay attention to `APPLICATION ID` listed in output because that is an
`id` used in most of the control commands to communicate to a specific
application instance. For example you may have multiple `XD YARN
runtime` instances running.
====

===== Kill application

Application can be killed using a `kill` command.

[source,bash]
----
$ bin/xd-yarn kill -a application_1420905836797_0001
Kill request for application_1420905836797_0001 sent
----


===== Using a built-in shell

To get a better and faster command usage a build-in shell can be used
to run control commands:

[source,bash]
----
$ bin/xd-yarn shell
Spring YARN Cli (v2.1.0.M3)
Hit TAB to complete. Type 'help' and hit RETURN for help, and 'exit' to quit.
$
clear            clustercreate    clusterdestroy   clusterinfo      clustermodify
clustersinfo     clusterstart     clusterstop      exit             help
kill             prompt           pushed           submit           submitted
$
----

[[connect-xd-shell-to-yarn-runtime-managed-admins]]
==== Connect xd-shell to YARN runtime managed admins

XD admins will register its runtime information into zookeeper and
you can use the `admininfo` command to query this information:

[source,bash]
----
$ bin/xd-yarn admininfo
Admins: [https://hadoop.localdomain:43740]
----

Then connect xd-shell to this instance:

[source,bash]
----
server-unknown:>admin config server --uri https://hadoop.localdomain:43740
Successfully targeted https://hadoop.localdomain:43740

xd:>runtime containers
  Container Id                          Host               IP Address      PID    Groups  Custom Attributes
  ------------------------------------  -----------------  --------------  -----  ------  --------------------------------------------------
 6324a9ae-205b-44b9-b851-f0edd7245286  node2.localdomain   172.16.101.102  12284  yarn    {virtualCores=1, memory=512, managementPort=54694}
----

[[configuring-yarn-memory-reservations]]
==== Configuring YARN memory reservations

YARN Nodemanager is continously tracking how much memory is used by
individual YARN containers. If containers are using more memory than
what the configuration allows, containers are simply killed by a
Nodemanager. Application master controlling the app lifecycle is given
a little more freedom meaning that Nodemanager is not that aggressive
when making a desicion when a container should be killed.

Lets take a quick look of memory related settings in YARN cluster and
in YARN applications. Below xml config is what a default vanilla Apache
Hadoop uses for memory related settings. Other distributions may have
different defaults.

[source,xml]
.yarn-site.xml
----
<configuration>

  <property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>true</value>
  </property>

  <property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>true</value>
  </property>

  <property>
    <name>yarn.nodemanager.vmem-pmem-ratio</name>
    <value>2.1</value>
  </property>

  <property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>1024</value>
  </property>

  <property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>8192</value>
  </property>

  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>8192</value>
  </property>

</configuration>
----

yarn.nodemanager.pmem-check-enabled::

Enables a check for physical memory of a process. This check if
enabled is directly tracking amount of memory requested for a YARN
container. 

yarn.nodemanager.vmem-check-enabled::

Enables a check for virtual memory of a process. This setting is one
which is usually causing containers of a custom YARN applications to
get killed by a node manager. Usually the actual ratio between
physical and virtual memory is higher than a default `2.1` or bugs in
a OS is causing wrong calculation of a used virtual memory.

yarn.nodemanager.vmem-pmem-ratio::

Defines a ratio of allowed virtual memory compared to physical memory.
This ratio simply defines how much virtual memory a process can use
but the actual tracked size is always calculated from a physical
memory limit.

yarn.scheduler.minimum-allocation-mb::

Defines a minimum allocated memory for container.

+
[NOTE]
====
This setting also indirectly defines what is the actual physical
memory limit requested during a container allocation. Actual physical
memory limit is always going to be multiple of this setting rounded to
upper bound. For example if this setting is left to default `1024` and
container is requested with `512M`, `1024M` is going to be used.
However if requested size is `1100M`, actual size is set to `2048M`.
====

yarn.scheduler.maximum-allocation-mb::

Defines a maximum allocated memory for container.

yarn.nodemanager.resource.memory-mb::

Defines how much memory a node controlled by a node manager is allowed
to allocate. This setting should be set to amount of which OS is able
give to YARN managed processes in a way which doesn't cause OS to
swap, etc.

[TIP]
====
If testing XD YARN runtime on a single computer with a multiple VM
based hadoop cluster a pro tip is to set both
`yarn.nodemanager.pmem-check-enabled` and
`yarn.nodemanager.vmem-check-enabled` to `false`, set
`yarn.scheduler.minimum-allocation-mb` much lower to either `256` or
`512` and `yarn.nodemanager.resource.memory-mb` 15%-20% below a
defined VM memory.
====

We have three memory settings for components participating `XD YARN
runtime`. You can use configuration properties
`spring.xd.appmasterMemory`, `spring.xd.adminMemory` and
`spring.xd.containerMemory` respectively.

[source,yaml]
----
spring:
  xd:
    appmasterMemory: 512M
    adminMemory: 512M
    containerMemory: 512M
----

[[working-with-container-groups]]
==== Working with container groups

Container grouping and clustering is more sophisticated feature which
allows better control of XD admins and containers at runtime. Basic
features are:

* Control members in a groups.
* Control lifecycle state for group as whole.
* Create groups dynamically.
* Re-start failed containers.

`XD YARN Runtime` has a few built-in groups to get you started. There
are two groups `admin` and `container` created by default which both
are lauching exactly one container chosen randomly from YARN cluster.



===== List existing groups

Run the command:

[source,bash]
----
$ bin/xd-yarn clustersinfo -a application_1420911708637_0001
  CLUSTER ID
  ----------
  container
  admin
----

===== Get status of a group

Run the command:

[source,bash]
----
bin/xd-yarn clusterinfo -a application_1420911708637_0001 -c admin
  CLUSTER STATE  MEMBER COUNT
  -------------  ------------
  RUNNING        1
----

Or to get verbose output:

----
$ bin/xd-yarn clusterinfo -a application_1420911708637_0001 -c admin -v
  CLUSTER STATE  MEMBER COUNT  ANY PROJECTION  HOSTS PROJECTION  RACKS PROJECTION  ANY SATISFY  HOSTS SATISFY  RACKS SATISFY
  -------------  ------------  --------------  ----------------  ----------------  -----------  -------------  -------------
  RUNNING        1             1               {}                {}                0            {}             {}
----

===== Control group state

Run the commands to stop group, list its status, start group and
finally list status again:

[source,bash]
----
$ bin/xd-yarn clusterinfo -a application_1420911708637_0001 -c container
  CLUSTER STATE  MEMBER COUNT
  -------------  ------------
  RUNNING        1

$ bin/xd-yarn clusterstop -a application_1420911708637_0001 -c container
Cluster container stopped.

$ bin/xd-yarn clusterinfo -a application_1420911708637_0001 -c container
  CLUSTER STATE  MEMBER COUNT
  -------------  ------------
  STOPPED        0

$ bin/xd-yarn clusterstart -a application_1420911708637_0001 -c container
Cluster container started.

$ bin/xd-yarn clusterinfo -a application_1420911708637_0001 -c container
  CLUSTER STATE  MEMBER COUNT
  -------------  ------------
  RUNNING        1
----

===== Modify group configuration

In these commans we first ramp up container count and then ramp it
down:

[source,bash]
----
18:19 $ bin/xd-yarn clusterinfo -a application_1420911708637_0001 -c container
  CLUSTER STATE  MEMBER COUNT
  -------------  ------------
  RUNNING        1

$ bin/xd-yarn clustermodify -a application_1420911708637_0001 -c container -w 3
Cluster container modified.

$ bin/xd-yarn clusterinfo -a application_1420911708637_0001 -c container
  CLUSTER STATE  MEMBER COUNT
  -------------  ------------
  RUNNING        3

$ bin/xd-yarn clustermodify -a application_1420911708637_0001 -c container -w 2
Cluster container modified.

$ bin/xd-yarn clusterinfo -a application_1420911708637_0001 -c container
  CLUSTER STATE  MEMBER COUNT
  -------------  ------------
  RUNNING        2
----

[NOTE]
====
In above example we used option `-w` which is a shortcut for defining
YARN allocation which uses a wildcard requests allowing containers to
be requested from any host.
====

===== Create a new group

When you want to create a new group that is because you need to
add new XD admin or container nodes to a current system with a
different settings. These setting usually differ by a colocation of
containers. More about built-in group configuration refer to section
<<built-in-group-configurations>>.

Run the command:

[source,bash]
----
$ bin/xd-yarn clustercreate -a application_1420911708637_0001 -c custom -i container-nolocality-template -p default -w 2
Cluster custom created.

$ bin/xd-yarn clusterinfo -a application_1420911708637_0001 -c custom
  CLUSTER STATE  MEMBER COUNT
  -------------  ------------
  INITIAL        0

$ bin/xd-yarn clusterstart -a application_1420911708637_0001 -c custom
Cluster custom started.

$ bin/xd-yarn clusterinfo -a application_1420911708637_0001 -c custom
  CLUSTER STATE  MEMBER COUNT
  -------------  ------------
  RUNNING        2
----

To create group with two containers on `node5` and one on `node6` run command:

[source,bash]
----
$ bin/xd-yarn clustercreate -a application_1420911708637_0001 -c custom -i container-locality-template -p default -y "{hosts:{node6: 1,node5: 2}}"
Cluster custom created.

$ bin/xd-yarn -a application_1420911708637_0001 -c custom -v
  CLUSTER STATE  MEMBER COUNT  ANY PROJECTION  HOSTS PROJECTION    RACKS PROJECTION  ANY SATISFY  HOSTS SATISFY       RACKS SATISFY
  -------------  ------------  --------------  ------------------  ----------------  -----------  ------------------  -------------
  INITIAL        0             0               {node5=2, node6=1}  {}                0            {node5=2, node6=1}  {}
----

===== Destroy a group

Run the commands:

[source,bash]
----
$ bin/xd-yarn clustersinfo -a application_1420911708637_0001
  CLUSTER ID
  ----------
  container
  admin

$ bin/xd-yarn clusterinfo -a application_1420911708637_0001 -c container
  CLUSTER STATE  MEMBER COUNT
  -------------  ------------
  RUNNING        1

$ bin/xd-yarn clusterstop -a application_1420911708637_0001 -c container
Cluster container stopped.

$ bin/xd-yarn clusterinfo -a application_1420911708637_0001 -c container
  CLUSTER STATE  MEMBER COUNT
  -------------  ------------
  STOPPED        0

$ bin/xd-yarn clusterdestroy -a application_1420911708637_0001 -c container
Cluster container destroyed.

$ bin/xd-yarn clustersinfo -a application_1420911708637_0001
  CLUSTER ID
  ----------
  admin
----

[NOTE]
====
Group can only destroyed if its status is `STOPPED` or `INITIAL`.
====

[[built-in-group-configurations]]
===== Built-in group configurations

Few groups are already defined where `admin` and `container` are
enabled automatically. Other groups are disabled and thus working as
a blueprints which can be used to create groups manually.

admin::
Default group definition for XD admins.

container::
Default group definition for XD containers.

admin-nolocality-template::
Blueprint with relax localization. Use this to create a groups if you
plan to use any matching.

admin-locality-template::
Blueprint with no relax localization. Use this to create a groups if
you plan to use hosts or racks matching.

container-nolocality-template::
Blueprint with relax localization. Use this to create a groups if you
plan to use any matching.

container-locality-template::
Blueprint with no relax localization. Use this to create a groups if
you plan to use hosts or racks matching.

==== Configuration examples

This section contains examples of usual use cases for custom
configurations.

===== Run containers on a specific hosts

Below configuration sets default XD container to exist on `node1` and
`node2`.

[source,yaml]
----
xd:
  containerLocality: true
spring:
  yarn:
    appmaster:
      containercluster:
        clusters:
          container:
            projection:
              data: {any: 0, hosts: {node1: 1, node2: 1}}
----

===== Run admins on a specific racks

Below configuration sets default XD admins to exist on `/rack1` and
`/rack2`.

[source,yaml]
----
xd:
  adminLocality: true
spring:
  yarn:
    appmaster:
      containercluster:
        clusters:
          admin:
            projection:
              data: {any: 0, racks: {/rack1: 1, /rack2: 1}}
----

===== Disable default admin and container groups

Existing built-in groups `admin` and `container` can be disabled by
setting their projection types to `null`.

[source,yaml]
----
spring:
  yarn:
    appmaster:
      containercluster:
        clusters:
          admin:
            projection:
              type: null
          container:
            projection:
              type: null
----

==== `xd-yarn` command synopsis

push::
+
[source,bash]
----
xd-yarn push - Push new application version

usage: xd-yarn push [options]

Option                     Description
------                     -----------
-v, --application-version  Application version (default: app)
----
+

pushed::
+
[source,bash]
----
xd-yarn pushed - List pushed applications

usage: xd-yarn pushed [options]

No options specified
----
+

submit::
+
[source,bash]
----
xd-yarn submit - Submit application

usage: xd-yarn submit [options]

Option                     Description
------                     -----------
-v, --application-version  Application version (default: app)
----
+

submitted::
+
[source,bash]
----
xd-yarn submitted - List submitted applications

usage: xd-yarn submitted [options]

Option                   Description
------                   -----------
-t, --application-type   Application type (default: XD)
-v, --verbose [Boolean]  Verbose output (default: true)
----

kill::
+
[source,bash]
----
xd-yarn kill - Kill application

usage: xd-yarn kill [options]

Option                Description
------                -----------
-a, --application-id  Specify YARN application id
----

clustersinfo::
+
[source,bash]
----
xd-yarn clustersinfo - List clusters

usage: xd-yarn clustersinfo [options]

Option                Description
------                -----------
-a, --application-id  Specify YARN application id
----

clusterinfo::
+
[source,bash]
----
xd-yarn clusterinfo - List cluster info

usage: xd-yarn clusterinfo [options]

Option                   Description
------                   -----------
-a, --application-id     Specify YARN application id
-c, --cluster-id         Specify cluster id
-v, --verbose [Boolean]  Verbose output (default: true)
----

clustercreate::
+
[source,bash]
----
xd-yarn clustercreate - Create cluster

usage: xd-yarn clustercreate [options]

Option                  Description
------                  -----------
-a, --application-id    Specify YARN application id
-c, --cluster-id        Specify cluster id
-g, --container-groups  Container groups
-h, --projection-hosts  Projection hosts counts
-i, --cluster-def       Specify cluster def id
-p, --projection-type   Projection type
-r, --projection-racks  Projection racks counts
-w, --projection-any    Projection any count
-y, --projection-data   Raw projection data
----

clusterdestroy::
+
[source,bash]
----
xd-yarn clusterdestroy - Destroy cluster

usage: xd-yarn clusterdestroy [options]

Option                Description
------                -----------
-a, --application-id  Specify YARN application id
-c, --cluster-id      Specify cluster id
----

clustermodify::
+
[source,bash]
----
xd-yarn clustermodify - Modify cluster

usage: xd-yarn clustermodify [options]

Option                  Description
------                  -----------
-a, --application-id    Specify YARN application id
-c, --cluster-id        Specify cluster id
-h, --projection-hosts  Projection hosts counts
-r, --projection-racks  Projection racks counts
-w, --projection-any    Projection any count
-y, --projection-data   Raw projection data
----

clusterstart::
+
[source,bash]
----
xd-yarn clusterstart - Start cluster

usage: xd-yarn clusterstart [options]

Option                Description
------                -----------
-a, --application-id  Specify YARN application id
-c, --cluster-id      Specify cluster id
----

clusterstop::
+
[source,bash]
----
xd-yarn clusterstop - Stop cluster

usage: xd-yarn clusterstop [options]

Option                Description
------                -----------
-a, --application-id  Specify YARN application id
-c, --cluster-id      Specify cluster id
----

==== Introduction to YARN resource allocation

This section describes some background of how YARN resource allocation
works, what are the limitations of it and more importantly how it
reflects into `XD YARN runtime`.

[NOTE]
====
More detailed info of resource allocation can be found from a `Spring
for Apache Hadoop` reference documentation.
====

YARN as having a strong roots from original MapReduce framework is
imposing relatively strange concepts of where containers are about to
be executed. In a MapReduce world every map and reduce tasks are
executed in its own container where colocation is usually determined
by a physical location of a HDFS file block map or reduce tasks are
accessing. This is introducing a concepts of allocating containers on
`any` hosts, specific `hosts` or specific `racks`. Usually YARN is
trying to place container as close as possible to a physical location
to minimize network IO so i.e. if host cannot be chosen, rack is
chosen instead assuming a whole rack is connected together with a fast
switch.

For custom YARN applications like `XD YARN runtime` this doesn't
necessarily make that much sense because we're not hard-tied to HDFS
file blocks. What makes sense is that we can still place containers on
different racks to get better high availability in case whole rack
goes down or if specific containers needs to exist on specific hosts
to access either custom physical or network resources. Good example of
having a need to execute something on a specific host is either a disk
access or outbound internet access if cluster is highly secured.

One other YARN resource allocation concept worth mentioning is
relaxation of container locality. This simply means that if resources
are requested from hosts or racks, YARN will relax those requests if
resources cannot be allocated immediately. Turning relax flag off
guarantees that containers will be allocated from hosts or racks.
Though these requests will then wait forever if allocation cannot be
done.

#!/bin/bash

. xdapi

PWD=`pwd`
# Base path in HDFS
NOW=`date "+%H-%M-%S"`
BASE_PATH="/xdtest/$NOW/"
TEST_DIR='/tmp/xdtest/batchhdfs'

wait_for_server

echo 'Hadoop environment and classpath:'
set | grep HADOOP
hadoop classpath

set -e

echo -e "\nRunning batch HDFS import tests"
echo "HDFS base path is $BASE_PATH"

echo -e "\n\n*** Test 1. Load single CSV file, no rollover\n"

create_job csvjob "filepollhdfs --names=col1,col2,col3 --directory=$BASE_PATH --fileName=blah --fileExtension=csv" 'true'
create_stream csvstream "file --ref=true --dir=$PWD/csv --pattern=data.csv > queue:job:csvjob" 'true'

sleep 10

blah_size=`hdfs_size "$BASE_PATH/blah-0.csv"`
destroy_stream csvstream
destroy_job csvjob
echo "Checking size of file blah-0.csv"
assert_equals 6358 $blah_size || exit $?

echo -e "\n\n*** Test 2. Import duplicate copies of a CSV data file\n"

copy_files() {
  if [[ ! -f $TEST_DIR/csv/data1.csv ]]
  then
    mkdir -p $TEST_DIR/csv
    for i in {1..4}
    do
      echo "Copying csv file $i to $TEST_DIR/csv"
      cp $PWD/csv/data.csv "$TEST_DIR/csv/data$i.csv"
    done
  fi
}

copy_files
create_job csvjob2 "filepollhdfs --deleteFiles=true --names=col1,col2,col3 --rollover=5000 --directory=$BASE_PATH --fileExtension=csv" 'true'
create_stream csvstream2 "file --ref=true --dir=$TEST_DIR/csv --pattern=*.csv > queue:job:csvjob2" 'true'

count_files() {
  local nCsvFiles=`ls -1 $TEST_DIR/csv | wc -l`
  echo "$nCsvFiles"
}

i=5
while [ $i != 0 ]
do
  nFiles=`count_files`
  if [ $nFiles != 0 ]
  then
    echo "Waiting for jobs to finish..."; sleep 5
    ((i--))
  else
    i=0
  fi
done


job2_size=`hdfs_size "$BASE_PATH/csvjob2*"`

destroy_stream csvstream2
destroy_job    csvjob2

nCsvFiles=`count_files`
assert_equals 0 $nCsvFiles

echo "Checking size of HDFS results matches imports from $TEST_DIR/csv"
assert_equals 25432 $job2_size

echo -e "\n\n*** Test 3. Import files using hdfs sink\n"

copy_files
create_stream csvstream3 "file --dir=$TEST_DIR/csv --pattern=data1.csv --outputType=text/plain | hdfs --directory=$BASE_PATH" 'true'
sleep 10
undeploy_stream csvstream3
deploy_stream csvstream3
sleep 10
destroy_stream csvstream3

data_size=`hdfs_size "$BASE_PATH/csvstream3*"`
assert_equals 12718 $data_size || exit $?

if [[ ! -f $XD_HOME/lib/sqlite-jdbc-3.7.2.jar ]]
then
  echo 'sqlite jar is missing from XD_HOME/lib. Skipping JDBC tests'
  exit 0
fi

echo -e "\n\n Test 4. Export file from hdfs to JDBC"

if [[ -f $TEST_DIR/hdfsjdbc.db ]]
then
  rm $TEST_DIR/hdfsjdbc.db &> /dev/null
fi

mkdir -p $TEST_DIR

sqlite3 $TEST_DIR/hdfsjdbc.db 'create table blah (col1 varchar, col2 varchar, col3 varchar)'

create_job csvjob4 "hdfsjdbc --driverClassName=org.sqlite.JDBC --url=jdbc:sqlite:$TEST_DIR/hdfsjdbc.db --resources=${BASE_PATH}blah-0.csv --names=col1,col2,col3 --tableName=blah --initializeDatabase=false" 'true'
launch_job csvjob4
sleep 15
rows=`sqlite3 $TEST_DIR/hdfsjdbc.db 'select count(*) from blah'`
destroy_job csvjob4
echo "Checking row count in database table matches import from hdfs..."
assert_equals 292 $rows

echo -e "\n\n Test 5. Import back to hdfs from JDBC"

create_job csvjob5 "jdbchdfs --driverClassName=org.sqlite.JDBC --url=jdbc:sqlite:$TEST_DIR/hdfsjdbc.db --directory=$BASE_PATH --fileName=jdbcblah --fileExtension=csv --columns=col1,col2,col3 --tableName=blah" 'true'
launch_job csvjob5
sleep 15

blah_size=`hdfs_size "$BASE_PATH/jdbcblah-0.csv"`
destroy_job csvjob5
echo "Checking size of file jdbcblah-0.csv"
assert_equals 6358 $blah_size

echo -e "\n\n Test 6. Import back to hdfs from JDBC using SQL"

create_job csvjob6 "jdbchdfs --driverClassName=org.sqlite.JDBC --url=jdbc:sqlite:$TEST_DIR/hdfsjdbc.db --directory=$BASE_PATH --fileName=jdbcblahsql --fileExtension=csv --sql='select col1,col2,col3 from blah'" 'true'
launch_job csvjob6
sleep 15

blah_size=`hdfs_size "$BASE_PATH/jdbcblahsql-0.csv"`
destroy_job csvjob6
echo "Checking size of file jdbcblahsql-0.csv"
assert_equals 6358 $blah_size


echo "All good :-) !!"


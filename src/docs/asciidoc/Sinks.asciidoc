
// Empty line above needed after the list from previous file
[[sinks]]
== Sinks

This section describes the _sink_ modules included with Spring XD. A sink terminates a stream to persist data or push it to an external consumer. To run the examples shown here, start the XD Container
as instructed in the xref:Getting-Started#getting-started[Getting Started] page.

The available sinks are

* <<dynamic-router, Dynamic Router>>
* <<file-sink, File>>
* <<ftp-sink, FTP>>
* <<gemfire-server, GemFire Server>>
* <<gpfdist, GPFDIST>>
* <<cassandra, Cassandra>>
* <<hadoop-hdfs, HDFS>>
* <<hdfs-dataset-avroparquet, HDFS Dataset>>
* <<jdbc, JDBC>>
* <<kafka-sink, Kafka>>
* <<log, Log>>
* <<mail, Mail>>
* <<mongo, Mongo>>
* <<mqtt-sink, MQTT>>
* <<null-sink, Null Sink>>
* <<rabbitmq, RabbitMQ>>
* <<redis, Redis>>
* <<shell-sink, Shell Command>>
* <<splunk-server, Splunk Server>>
* <<tcp-sink, TCP>>


Additionally, Spring XD provides a number of xref:Counters-and-Gauges#counters-and-gauges[counters and gauges], which are specialized sinks useful for real time analytics.

See the section xref:Creating-a-Sink-Module#creating-a-sink-module[Creating a Sink Module] for information on how to create sink modules using other Spring Integration Adapters.

[[dynamic-router]]
=== Dynamic Router (`router`)

The Dynamic Router support allows for routing Spring XD messages to *named channels* based on the evaluation of SpEL expressions or Groovy Scripts.

==== SpEL-based Routing

In the following example, 2 streams are created that listen for message on the *foo* and the *bar* channel. Furthermore, we create a stream that receives messages via HTTP and then delegates the received messages to a router:

----
xd:>stream create f --definition "queue:foo > transform --expression=payload+'-foo' | log" --deploy
Created new stream 'f'

xd:>stream create b --definition "queue:bar > transform --expression=payload+'-bar' | log" --deploy
Created new stream 'b'

xd:>stream create r --definition "http | router --expression=payload.contains('a')?'queue:foo':'queue:bar'" --deploy
Created new stream 'r'
----

Now we make 2 requests to the HTTP source:

----
xd:>http post --data "a"
> POST (text/plain;Charset=UTF-8) http://localhost:9000 a
> 200 OK

xd:>http post --data "b"
> POST (text/plain;Charset=UTF-8) http://localhost:9000 b
> 200 OK
----

In the server log you should see the following output:

----
11:54:19,868  WARN ThreadPoolTaskScheduler-1 sink.f:145 - a-foo
11:54:25,669  WARN ThreadPoolTaskScheduler-1 sink.b:145 - b-bar
----

For more information, please also consult the Spring Integration Reference manual: https://docs.spring.io/spring-integration/reference/html/messaging-routing-chapter.html#router-namespace particularly the section "Routers and the Spring Expression Language (SpEL)".

==== Groovy-based Routing

Instead of SpEL expressions, Groovy scripts can also be used. Let's create a Groovy script in the file system at "/my/path/router.groovy"

[source,groovy]
----
println("Groovy processing payload '" + payload +"'");
if (payload.contains('a')) {
	return ":foo"
}
else {
	return ":bar"
}
----

Now we create the following streams:

----
xd:>stream create f --definition ":foo > transform --expression=payload+'-foo' | log" --deploy
Created new stream 'f'

xd:>stream create b --definition ":bar > transform --expression=payload+'-bar' | log" --deploy
Created new stream 'b'

xd:>stream create g --definition "http | router --script='file:/my/path/router.groovy'" --deploy
----

Now post some data to the HTTP source:

----
xd:>http post --data "a"
> POST (text/plain;Charset=UTF-8) http://localhost:9000 a
> 200 OK

xd:>http post --data "b"
> POST (text/plain;Charset=UTF-8) http://localhost:9000 b
> 200 OK
----

In the server log you should see the following output:

----
Groovy processing payload 'a'
11:29:27,274  WARN ThreadPoolTaskScheduler-1 sink.f:145 - a-foo
Groovy processing payload 'b'
11:34:09,797  WARN ThreadPoolTaskScheduler-1 sink.b:145 - b-bar
----

[NOTE]
===============================
You can also use Groovy scripts located on your classpath by specifying:
----
--script='org/my/package/router.groovy'
----
===============================

If you want to pass variable values to your script, you can statically bind values using the _variables_ option or optionally pass the path to a properties file containing the bindings using the _propertiesLocation_ option. All properties in the file will be made available to the script as variables. You may specify both _variables_ and _propertiesLocation_, in which case any duplicate values provided as _variables_ override values provided in _propertiesLocation_. Note that _payload_ and _headers_ are implicitly bound to give you access to the data contained in a message.

For more information, see the Spring Integration Reference manual: "Groovy support"
https://docs.spring.io/spring-integration/reference/html/messaging-endpoints-chapter.html#groovy

==== Options

//^sink.router
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.router' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$router$$** $$sink$$ has the following options:

$$expression$$:: $$a SpEL expression used to transform messages$$ *($$String$$, default: `payload.toString()`)*
$$propertiesLocation$$:: $$the path of a properties file containing custom script variable bindings$$ *($$String$$, no default)*
$$refreshDelay$$:: $$how often to check (in milliseconds) whether the script has changed; -1 for never$$ *($$long$$, default: `60000`)*
$$script$$:: $$reference to a script used to process messages$$ *($$String$$, no default)*
$$variables$$:: $$variable bindings as a comma delimited string of name-value pairs, e.g., 'foo=bar,baz=car'$$ *($$String$$, no default)*
//$sink.router

TIP: If the `script` option is set, the script file's modified timestamp is checked for changes every 60 seconds by
default; this can be changed with the `refreshDelay` deployment property: `--refreshDelay=30000` (every 30 seconds or
30,000ms), `--refreshDelay=-1` to disable refresh.

[[file-sink]]
=== File Sink (`file`)

Another simple option is to stream data to a file on the host OS. This can be done using the `file` sink module to create a xref:Streams#streams[stream].

  xd:> stream create --name myfilestream --definition "http --port=8000 | file" --deploy

We've used the `http` source again, so run the following command to send a message

  xd:> http post --target http://localhost:8000 --data "hello"

The `file` sink uses the stream name as the default name for the file it creates, and places the file in the `/tmp/xd/output/` directory.

[source,bash]
----
$ less /tmp/xd/output/myfilestream
hello
----

You can cutomize the behavior and specify the `name` and `dir` options of the output file. For example

  xd:> stream create --name otherfilestream --definition "http --port=8000 | file --name=myfile --dir=/some/custom/directory" --deploy

To set the filename from a SpEL expression (e.g. headers['file_name']), you can use the `nameExpression` option.

  xd:> stream create --name myfilestream --definition "http --port=8000 | file --nameExpression=payload.trim()" --deploy

If you run this command :

  xd:> http post --target http://localhost:8000 --data "hello.txt"

It will take the payload of the message ("hello.txt"), as it's defined previously with `nameExpression`, and use it
as the filename. In this example, the filename is equal to the content of the file.

You can use `dirExpression` to specify the name of the directory that will contain the new file.

  xd:> stream create --name myfilestream --definition "http --port=8000 | file --nameExpression=payload.trim() --dirExpression='''/tmp/test/dir-'' + payload.trim()'" --deploy

If you run this command :

  xd:> http post --target http://localhost:8000 --data "hello.txt"

For the filename, it will do the same thing as explained previously. For the directory name it will use
the content of the file (trimmed) concatenated with 'dir-' (in that case : "/tmp/test/dir-hello.txt").
If the destination directory does not exists, the respective destination directory and any non-existing
parent directories are being created automatically.

When you use the `nameExpression` option you have to use the `dirExpression` option (not the `dir` option) to
specify the destination directory name, even if it's a simple string (e.g. `'mydir'`).

==== File with Options
//^sink.file
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.file' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$file$$** $$sink$$ has the following options:

$$binary$$:: $$if false, will append a newline character at the end of each line$$ *($$boolean$$, default: `false`)*
$$charset$$:: $$the charset to use when writing a String payload$$ *($$String$$, default: `UTF-8`)*
$$dir$$:: $$the directory in which files will be created$$ *($$String$$, default: `/tmp/xd/output/`)*
$$dirExpression$$:: $$spring expression used to define directory name$$ *($$String$$, no default)*
$$mode$$:: $$what to do if the file already exists$$ *($$Mode$$, default: `APPEND`, possible values: `APPEND,REPLACE,FAIL,IGNORE`)*
$$name$$:: $$filename pattern to use$$ *($$String$$, default: `<stream name>`)*
$$nameExpression$$:: $$spring expression used to define filename$$ *($$String$$, no default)*
$$suffix$$:: $$filename extension to use$$ *($$String$$, no default)*
//$sink.file

[[ftp-sink]]
=== FTP Sink (`ftp`)

FTP sink is a simple option to push files to an FTP server from incoming messages.

It uses an `ftp-outbound-adapter`, therefore incoming messages could be either a `java.io.File` object, a `String` (content of the file)
or an array of `bytes` (file content as well).

To use this sink, you need a username and a password to login. Once you have this you can stream
data from, for instance, a file source to the ftp sink:

  xd:> stream create --name mystream --definition "file | ftp --username=me --password=mypwd" --deploy

We use the file source, so create a file:

[source,bash]
----
$ echo hello > /tmp/xd/input/mystream/test.txt
----

On the ftp server, you should see the file `test.txt` with the content `hello`.

To pass the filename to the module you can use the header `file_name` with the filename you wish to be used.

NOTE:
By default Spring Integration will use `o.s.i.file.DefaultFileNameGenerator` if none is specified. `DefaultFileNameGenerator` will determine the file name
based on the value of the `file_name` header (if it exists) in the `MessageHeaders`, or if the payload of the `Message` is already a `java.io.File`, then it will
use the original name of that file.

==== FTP with Options
//^sink.ftp
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.ftp' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$ftp$$** $$sink$$ has the following options:

$$autoCreateDir$$:: $$remote directory must be auto created if it does not exist$$ *($$boolean$$, default: `true`)*
$$clientMode$$:: $$client mode to use: 2 for passive mode and 0 for active mode$$ *($$int$$, default: `0`)*
$$host$$:: $$the host name for the FTP server$$ *($$String$$, default: `localhost`)*
$$mode$$:: $$what to do if the file already exists$$ *($$Mode$$, default: `REPLACE`, possible values: `APPEND,REPLACE,FAIL,IGNORE`)*
$$password$$:: $$the password for the FTP connection$$ *($$Password$$, no default)*
$$port$$:: $$the port for the FTP server$$ *($$int$$, default: `21`)*
$$remoteDir$$:: $$the remote directory to transfer the files to$$ *($$String$$, default: `/`)*
$$remoteFileSeparator$$:: $$file separator to use on the remote side$$ *($$String$$, default: `/`)*
$$temporaryRemoteDir$$:: $$temporary remote directory that should be used$$ *($$String$$, default: `/`)*
$$tmpFileSuffix$$:: $$extension to use on server side when uploading files$$ *($$String$$, default: `.tmp`)*
$$useTemporaryFilename$$:: $$use a temporary filename while transferring the file and rename it to its final name once it's fully transferred$$ *($$boolean$$, default: `true`)*
$$username$$:: $$the username for the FTP connection$$ *($$String$$, no default)*
//$sink.ftp

[[gemfire-server]]
=== GemFire Server

Currently XD supports GemFire's client-server topology. A sink that writes data to a GemFire cache requires at least one  cache server to be running in a separate process and may also be configured to use a Locator. While Gemfire configuration is outside of the scope of this document, details are covered in the http://docs.gopivotal.com/gemfire/index.html[GemFire Product documentation]. The XD distribution includes a standalone GemFire server executable suitable for development and test purposes and bootstrapped using a Spring configuration file provided as a command line argument. The GemFire jar is distributed freely under GemFire's development license and is subject to the license's terms and conditions. Sink modules provided with the XD distrubution that write data to GemFire create a client cache and client region. No data is cached on the client.

TIP: If native gemfire properties are required to configure the client cache, e.g., for security, place a `gemfire.properties` file in $XD_HOME/config.

==== Launching the XD GemFire Server

To start the GemFire cache server GemFire Server included in the Spring XD distribution, go to the XD install directory:

   $cd gemfire/bin
   $./gemfire-server ../config/cq-demo.xml

The command line argument is the path of a Spring Data Gemfire configuration file with including a configured cache server and one or more regions. A sample cache configuration is provided https://github.com/SpringSource/spring-xd/blob/master/spring-xd-gemfire-server/config/cq-demo.xml[cq-demo.xml] located in the `config` directory. Note that Spring interprets the path as a relative path unless it is explicitly preceded by `file:`. The sample configuration starts a server on port 40404 and creates a region named _Stocks_.

==== Gemfire sinks

There are 2 implementations of the gemfire sink: `gemfire-server` and `gemfire-json-server`. They are identical except the latter converts JSON string payloads to a JSON document format proprietary to GemFire and provides JSON field access and query capabilities. If you are not using JSON, the gemfire-server module will write the payload using java serialization to the configured region. Both modules accept the same options.

//^sink.gemfire-server
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.gemfire-server' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$gemfire-server$$** $$sink$$ has the following options:

$$host$$:: $$host name of the cache server or locator (if useLocator=true). May be a comma delimited list$$ *($$String$$, no default)*
$$keyExpression$$:: $$a SpEL expression which is evaluated to create a cache key$$ *($$String$$, default: `'<stream name>'`)*
$$port$$:: $$port of the cache server or locator (if useLocator=true). May be a comma delimited list$$ *($$String$$, no default)*
$$regionName$$:: $$name of the region to use when storing data$$ *($$String$$, default: `<stream name>`)*
$$useLocator$$:: $$indicates whether a locator is used to access the cache server$$ *($$boolean$$, default: `false`)*
//$sink.gemfire-server

TIP: The `keyExpression`, as its name suggests, is a SpEL. Typically, the key value is derived from the payload. The default of `'<streamname>'` (mind the quotes), will overwrite the same entry for every message received on the stream.

NOTE: The `useLocator` option is intended for integration with an existing GemFire installation in which the cache servers are configured to use locators in accordance with best practice. GemFire supports configuration of multiple locators (or direct server connections) and this is specified by supplying comma-delimited values for the `host` and `port` options. You may specify a single value for either of these options otherwise each value must contain the same size list. The following are examples are valid for multiple connection addresses:

    gemfire-server --host=myhost --port=10334,10335
    gemfire server --host=myhost1,myhost2 --port=10334
    gemfire server --host=myhost1,myhost2,myhost3 --port=10334,10335,10336

The last example creates connections to myhost1:10334, myhost2:10335, myhost3:10336

NOTE: You may also configure default Gemfire connection settings for all gemfire modules in `config\modules.yml`:

    gemfire:
       useLocator: true
       host: myhost1,myhost2
       port: 10334

==== Example
Suppose we have a JSON document containing a stock price:

      {"symbol":"FAKE", "price":73}

We want this to be cached using the stock symbol as the key. The stream definition is:

     http | gemfire-json-server --regionName=Stocks --keyExpression=payload.getField('symbol')

The keyExpression is a SpEL expression that depends on the payload type. In this case, _com.gemstone.org.json.JSONObject. JSONObject_ which  provides the _getField_ method. To run this example:

    xd:> stream create --name stocks --definition "http --port=9090 | gemfire-json-server --regionName=Stocks --keyExpression=payload.getField('symbol')" --deploy

    xd:> http post --target http://localhost:9090 --data {"symbol":"FAKE","price":73}

This will write an entry to the GemFire _Stocks_ region with the key _FAKE_.  Please do not put spaces when separating the JSON key-value pairs, only a comma.

You should see a message on STDOUT for the process running the GemFire server like:

    INFO [LoggingCacheListener] - updated entry FAKE

TIP: If you are deploying on Java 7 or earlier and need to deploy more than 4 Gemfire modules, be sure to increase the permsize of the singlenode or container. i.e. JAVA_OPTS="-XX:PermSize=256m".

[[gpfdist]]
=== GPFDIST

The gpfdist sink allows you to stream data in parallel to either Pivotal Greenplum DB
 or Pivotal HAWQ.  Internally, this sink creates a custom http listener that supports
the `gpfdist` protcol and schedules a task that orchestrates a `gploadd` session in the
same way it is done natively in Greenplum.

No data is written into temporary files and all data is kept in stream buffers waiting
to get inserted into Greenplum DB or HAWQ.  If there are no existing load sessions from Greenplum,
the sink will block until such sessions are established.

==== Example usage

The https://github.com/spring-projects/spring-xd-modules/tree/master/load-generator-gpfdist-source[load-generator-gpfdist] source can be used to send dummy test data to the `gpfdist` sink.

Using `psql`, create the following table with a simple schema that matches the data produced by the
`load-generator-string` source, two integer values, a producer ID and a timestamp separated by a tab.

[source,text]
----
create table xdsink (date integer, time integer) distributed randomly;
----

Now create the stream definition and deploy.  You should ensure that your pg_hba.conf (e.g. /data/master/gpsne-1/pg_hba.conf) is configured to allow a connection from your host where you are running the gpfdist sink. (an entry such as `host     all         gpadmin         192.168.70.128/32	trust`)

[source,text]
----
xd:>stream create --name gpfdiststream --definition "load-generator-gpfdist --messageCount=10000000 --producers=1 --recordType=counter  | gpfdist --dbHost=192.168.70.138 --table=xdsink --batchTimeout=5 --batchCount=1000 --batchPeriod=0 --flushCount=200 --flushTime=2 --rateInterval=1000000" --deploy
Created and deployed new stream 'gpfdiststream'
----

In this XD stream we send 10M messages from the `load-generator-string` source to the `gpfdist` sink.
We roughly keep load session alive for 5 seconds while flushing data after 2s or 200 entries which ever
comes first and sleep 0s in between load sessions.

You will see log output (you will probably need to set the log level of the package log4j.logger.org.springframework.xd.greenplum to INFO.)
[source,text]
----
2015-05-14 22:48:23,669 1.2.0.SNAP  INFO pool-14-thread-1 gpfdist.GPFDistMessageHandler - METER: 1 minute rate = 200000.0 mean rate = 269618.7284878825
2015-05-14 22:48:25,495 1.2.0.SNAP  INFO sqlTaskScheduler-1 support.CleanableJdbcOperations - DROP EXTERNAL TABLE xdsink_ext_fabcf3bb_c514_49ca_bfd6_cacb009463dc
2015-05-14 22:48:25,498 1.2.0.SNAP  INFO sqlTaskScheduler-1 support.CleanableJdbcOperations - CREATE READABLE EXTERNAL TABLE xdsink_ext_ae89e85d_eb65_4e11_ad72_4b8302086ebd ( LIKE xdsink ) LOCAT
----

`gpfdist` sink currently contains a throughput meter for this POC to
get perf numbers. In this case it is showing about 270K/sec
messages per second to be transferred from XD into Greenplum.

In previous example we did a simple inserts into a table. Let's see
how we can update data in a table. Create a simple table `xdsink2`
with three text columns and insert some data.

[source,text]
----
create table xdsink2 (col1 text, col2 text, col3 text);
insert into xdsink2 values ('DATA1', 'DATA', 'DATA');
insert into xdsink2 values ('DATA2', 'DATA', 'DATA');
insert into xdsink2 values ('DATA3', 'DATA', 'DATA');
----

We'd have different data in `col1` which is used to match data to be
updated.

[source,text]
----
# select * from xdsink2;
 col1  | col2 | col3
-------+------+------
 DATA3 | DATA | DATA
 DATA2 | DATA | DATA
 DATA1 | DATA | DATA
----

Let's create a stream which will update table `xdsink2` by matching a
column `col1` and updates columns `col2` and `col3`.

[source,text]
----
xd:>stream create --name gpfdiststream2 --definition "http|gpfdist --mode=UPDATE --table=xdsink2 --dbHost=mdw --columnDelimiter=',' --matchColumns=col1 --updateColumns=col2,col3" --deploy
----

Post some data into a stream which will be passed into a `gpfdist`
sink via `http` source.

[source,text]
----
xd:>http post --data "DATA1,DATA1,DATA1"
----

If you query table again, you'll see that row for `DATA1` has been
updated.

[source,text]
----
# select * from xdsink2;
 col1  | col2  | col3
-------+-------+-------
 DATA3 | DATA  | DATA
 DATA2 | DATA  | DATA
 DATA1 | DATA1 | DATA1
----

==== Performance Notes

On a Lenovo W540, Spring XD singlenode, `load-generator-string | gpfdist` inserted data at ~ 540K/sec.
The underlying message handler in the gpfdist sink is able to achieve ~1.2M/sec, which is comprable to
the use of the native gpload client.  Additional performance optimizations when used within an XD stream
are on the roadmap.

==== Implementation Notes

Within a `gpfdist` sink we have a Reactor based stream where data is published from the incoming SI channel.
This channel receives data from the Message Bus.  The Reactor stream is then connected to `Netty` based
http channel adapters so that when a new http connection is established, the Reactor stream is flushed and balanced among
existing http clients.  When `Greenplum` does a load from an external table, each segment will initiate
a http connection and start loading data.  The net effect is that incoming data is automatically spread
among the Greenplum segments.

==== GPFDIST with Options

The options `flushCount` and `flushTime` are used to determine when to flush
data that is buffered in an internal
https://projectreactor.io/docs/reference/streams.html#basics[Reactor stream] to
the http connection.  Data is flushed based on if the count value has been reached
or the time specified has elapsed.  Note that with too high a value, memory consumption
will go up.  Too small a value combined with a low ingestion rate will result in data
being inserted into the database less frequently.

`batchCount` defines the maximum count of aggregated windows the client
takes before the internal Reactor stream and http channel is closed.

`batchTimeout` defines how many seconds each http connection should be
kept alive if no data is streamed to a client. Use this together with
`batchCount` to estimate how long each loading session should last.

`batchPeriod` defines how many seconds a task running load operation
should sleep in between a loads.

`mode` defines a database load logic which is either `INSERT` or
`UPDATE`. `INSERT` is a default mode. Similar to control file
`GPLOAD.OUTPUT.MODE` property. `MERGE` is not currently supported.

`columnDelimiter` defines a data delimiter character within a line of
data. Defaults to tabulator character. Similar to control file
`GPLOAD.SOURCE.DELIMITER` property.

`updateColumns` defines updated column names and required with mode
`UPDATE`. Similar to control file `GPLOAD.OUTPUT.UPDATE_COLUMNS` property.

`matchColumns` defines matched column names and required with mode
`UPDATE`. Similar to control file `GPLOAD.OUTPUT.MATCH_COLUMNS` property.

`sqlBefore` defines a simple sql clause to be run before of every load
operation. Similar to control file `GPLOAD.SQL.BEFORE` property.

`sqlAfter` defines a simple sql clause to be run after of every load
operation. Similar to control file `GPLOAD.SQL.AFTER` property.

`delimiter` is used to postfix incoming data with a line termination
because Greenplum expects line terminated data.

`controlFile` can be used to introduce more parameters for a load
operation. For simple use cases, the `table` property can be used.

`rateInterval` if set, enables rate logging passing through sink.

//^sink.gpfdist
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.gpfdist' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$gpfdist$$** $$sink$$ has the following options:

$$batchCount$$:: $$batch count$$ *($$int$$, default: `100`)*
$$batchPeriod$$:: $$batch period$$ *($$int$$, default: `10`)*
$$batchTimeout$$:: $$batch timeout$$ *($$int$$, default: `4`)*
$$columnDelimiter$$:: $$column delimiter$$ *($$Character$$, no default)*
$$controlFile$$:: $$path to yaml control file$$ *($$String$$, no default)*
$$dbHost$$:: $$database host$$ *($$String$$, default: `localhost`)*
$$dbName$$:: $$database name$$ *($$String$$, default: `gpadmin`)*
$$dbPassword$$:: $$database password$$ *($$String$$, default: `gpadmin`)*
$$dbPort$$:: $$database port$$ *($$int$$, default: `5432`)*
$$dbUser$$:: $$database user$$ *($$String$$, default: `gpadmin`)*
$$delimiter$$:: $$data line delimiter$$ *($$String$$, default: `
`)*
$$flushCount$$:: $$flush item count$$ *($$int$$, default: `100`)*
$$flushTime$$:: $$flush item time$$ *($$int$$, default: `2`)*
$$matchColumns$$:: $$match columns with update$$ *($$String$$, no default)*
$$mode$$:: $$mode, either insert or update$$ *($$String$$, no default)*
$$port$$:: $$gpfdist listen port$$ *($$int$$, default: `0`)*
$$rateInterval$$:: $$enable transfer rate interval$$ *($$int$$, default: `0`)*
$$sqlAfter$$:: $$sql to run after load$$ *($$String$$, no default)*
$$sqlBefore$$:: $$sql to run before load$$ *($$String$$, no default)*
$$table$$:: $$target database table$$ *($$String$$, no default)*
$$updateColumns$$:: $$update columns with update$$ *($$String$$, no default)*
//$sink.gpfdist

[[cassandra]]
=== Cassandra

The Cassandra sink write into a Cassandra table.  Here is a simple example

  xd:>stream create cassandraTest --definition "http | cassandra --initScript=file:<absolut-path-to>/int-db.cql --ingestQuery='insert into book (isbn, title, author) values (uuid(), ?, ?)'" --deploy

Where `int-db.cql` looks like:

[source,text]
----
DROP TABLE IF EXISTS book;

CREATE TABLE book  (
    isbn        uuid PRIMARY KEY,
    author      text,
    instock     boolean,
    pages       int,
    saledate    timestamp,
    title       text
);
----

You can then send data to this stream via

  xd:>http post --data "{\"title\": \"The Art of War\", \"author\": \"Sun Tzu\"}" --target http://localhost:9000

and see the table contents using the CQL

  SELECT * FROM cassandratest.book;

Another Insert example using a domain 'Book' object is

  xd:>stream create cassandraTest2 --definition "http | transform --expression='new org.springframework.xd.test.domain.Book(T(java.util.UUID).randomUUID(), #jsonPath(payload, \"$.title\"), #jsonPath(payload, \"$.author\"))' | cassandra --keyspace=cassandraTest" --deploy

  http post --data "{\"title\": \"The Art of War\", \"author\": \"Sun Tzu\"}" --target http://localhost:9000

You must place your domain object into the xd/lib directory, since some Spring Data comons classes are in the `xd/lib` directory and loaded into root classpath, not the module's classpath.

//^sink.cassandra
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.cassandra' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$cassandra$$** $$sink$$ has the following options:

$$compressionType$$:: $$the compression to use for the transport$$ *($$CompressionType$$, default: `NONE`, possible values: `NONE,SNAPPY`)*
$$consistencyLevel$$:: $$the consistencyLevel option of WriteOptions$$ *($$ConsistencyLevel$$, no default, possible values: `ANY,ONE,TWO,THREE,QUOROM,LOCAL_QUOROM,EACH_QUOROM,ALL,LOCAL_ONE,SERIAL,LOCAL_SERIAL`)*
$$contactPoints$$:: $$the comma-delimited string of the hosts to connect to Cassandra$$ *($$String$$, default: `localhost`)*
$$entityBasePackages$$:: $$the base packages to scan for entities annotated with Table annotations$$ *($$String[]$$, default: `[]`)*
$$ingestQuery$$:: $$the ingest Cassandra query$$ *($$String$$, no default)*
$$initScript$$:: $$the path to file with CQL scripts (delimited by ';') to initialize keyspace schema$$ *($$String$$, no default)*
$$keyspace$$:: $$the keyspace name to connect to$$ *($$String$$, default: `<stream name>`)*
$$metricsEnabled$$:: $$enable/disable metrics collection for the created cluster$$ *($$boolean$$, default: `true`)*
$$password$$:: $$the password for connection$$ *($$String$$, no default)*
$$port$$:: $$the port to use to connect to the Cassandra host$$ *($$int$$, default: `9042`)*
$$queryType$$:: $$the queryType for Cassandra Sink$$ *($$Type$$, default: `INSERT`, possible values: `INSERT,UPDATE,DELETE,STATEMENT`)*
$$retryPolicy$$:: $$the retryPolicy  option of WriteOptions$$ *($$RetryPolicy$$, no default, possible values: `DEFAULT,DOWNGRADING_CONSISTENCY,FALLTHROUGH,LOGGING`)*
$$statementExpression$$:: $$the expression in Cassandra query DSL style$$ *($$String$$, no default)*
$$ttl$$:: $$the time-to-live option of WriteOptions$$ *($$int$$, default: `0`)*
$$username$$:: $$the username for connection$$ *($$String$$, no default)*
//$sink.cassandra


[[hadoop-hdfs]]
=== Hadoop (HDFS) (`hdfs`)


If you do not have Hadoop installed, you can install Hadoop as described in our xref:Hadoop-Installation#installing-hadoop[separate guide]. Spring XD supports 4 Hadoop distributions, see xref:Running-Distributed-Mode#using-hadoop[using Hadoop] for more information on how to start Spring XD to target a specific distribution.

Once Hadoop is up and running, you can then use the `hdfs` sink when creating a xref:Streams#streams[stream]

  xd:> stream create --name myhdfsstream1 --definition "time | hdfs" --deploy

In the above example, we've scheduled `time` source to automatically send ticks to `hdfs` once in every second. If you wait a little while for data to accumuluate you can then list can then list the files in the hadoop filesystem using the shell's built in hadoop fs commands.  Before making any access to HDFS in the shell you first need to configure the shell to point to your name node.  This is done using the `hadoop config` command.

    xd:>hadoop config fs --namenode hdfs://localhost:8020

In this example the hdfs protocol is used but you may also use the webhdfs protocol.  Listing the contents in the output directory (named by default after the stream name) is done by issuing the following command.

  xd:>hadoop fs ls /xd/myhdfsstream1
  Found 1 items
  -rw-r--r--   3 jvalkealahti supergroup          0 2013-12-18 18:10 /xd/myhdfsstream1/myhdfsstream1-0.txt.tmp

While the file is being written to it will have the `tmp` suffix.  When the data written exceeds the rollover size (default 1GB) it will be renamed to remove the `tmp` suffix.  There are several options to control the in use file file naming options.  These are `--inUsePrefix` and `--inUseSuffix` set the file name prefix and suffix respectfully.

When you destroy a stream

  xd:>stream destroy --name myhdfsstream1

and list the stream directory again, in use file suffix doesn't exist anymore.

  xd:>hadoop fs ls /xd/myhdfsstream1
  Found 1 items
  -rw-r--r--   3 jvalkealahti supergroup        380 2013-12-18 18:10 /xd/myhdfsstream1/myhdfsstream1-0.txt

To list the list the contents of a file directly from a shell execute the hadoop cat command.

  xd:> hadoop fs cat /xd/myhdfsstream1/myhdfsstream1-0.txt
  2013-12-18 18:10:07
  2013-12-18 18:10:08
  2013-12-18 18:10:09
  ...

In the above examples we didn't yet go through why the file was written in a specific directory and why it was named in this specific way. Default location of a file is defined as `/xd/<stream name>/<stream name>-<rolling part>.txt`. These can be changed using options `--directory` and `--fileName` respectively. Example is shown below.

  xd:>stream create --name myhdfsstream2 --definition "time | hdfs --directory=/xd/tmp --fileName=data" --deploy
  xd:>stream destroy --name myhdfsstream2
  xd:>hadoop fs ls /xd/tmp
  Found 1 items
  -rw-r--r--   3 jvalkealahti supergroup        120 2013-12-18 18:31 /xd/tmp/data-0.txt

It is also possible to control the size of a files written into HDFS. The `--rollover` option can be used to control when file currently being written is rolled over and a new file opened by providing the rollover size in bytes, kilobytes, megatypes, gigabytes, and terabytes.

  xd:>stream create --name myhdfsstream3 --definition "time | hdfs --rollover=100" --deploy
  xd:>stream destroy --name myhdfsstream3
  xd:>hadoop fs ls /xd/myhdfsstream3
  Found 3 items
  -rw-r--r--   3 jvalkealahti supergroup        100 2013-12-18 18:41 /xd/myhdfsstream3/myhdfsstream3-0.txt
  -rw-r--r--   3 jvalkealahti supergroup        100 2013-12-18 18:41 /xd/myhdfsstream3/myhdfsstream3-1.txt
  -rw-r--r--   3 jvalkealahti supergroup        100 2013-12-18 18:41 /xd/myhdfsstream3/myhdfsstream3-2.txt

Shortcuts to specify sizes other than bytes are written as `--rollover=64M`, `--rollover=512G` or `--rollover=1T`.

The stream can also be compressed during the write operation. Example of this is shown below.

  xd:>stream create --name myhdfsstream4 --definition "time | hdfs --codec=gzip" --deploy
  xd:>stream destroy --name myhdfsstream4
  xd:>hadoop fs ls /xd/myhdfsstream4
  Found 1 items
  -rw-r--r--   3 jvalkealahti supergroup         80 2013-12-18 18:48 /xd/myhdfsstream4/myhdfsstream4-0.txt.gzip

From a native os shell we can use hadoop's fs commands and pipe data into gunzip.

  # bin/hadoop fs -cat /xd/myhdfsstream4/myhdfsstream4-0.txt.gzip | gunzip
  2013-12-18 18:48:10
  2013-12-18 18:48:11
  ...

Often a stream of data may not have a high enough rate to roll over files frequently, leaving the file in an opened state.  This prevents users from reading a consistent set of data when running mapreduce jobs.  While one can alleviate this problem by using a small rollover value, a better way is to use the `idleTimeout`  option that will automatically close the file if there was no writes during the specified period of time.   This feature is also useful in cases where burst of data is written into a stream and you'd like that data to become visible in HDFS.

NOTE: The `idleTimeout` value should not exceed the timeout values set on the Hadoop cluster. These are typically configured using the `dfs.socket.timeout` and/or `dfs.datanode.socket.write.timeout` properties in the `hdfs-site.xml` configuration file.

  xd:> stream create --name myhdfsstream5 --definition "http --port=8000 | hdfs --rollover=20 --idleTimeout=10000" --deploy

In the above example we changed a source to `http` order to control what we write into a `hdfs` sink. We defined a small rollover size and a timeout of 10 seconds. Now we can simply post data into this stream via source end point using a below command.

  xd:> http post --target http://localhost:8000 --data "hello"

If we repeat the command very quickly and then wait for the timeout we should be able to see that some files are closed before rollover size was met and some were simply rolled because of a rollover size.

  xd:>hadoop fs ls /xd/myhdfsstream5
  Found 4 items
  -rw-r--r--   3 jvalkealahti supergroup         12 2013-12-18 19:02 /xd/myhdfsstream5/myhdfsstream5-0.txt
  -rw-r--r--   3 jvalkealahti supergroup         24 2013-12-18 19:03 /xd/myhdfsstream5/myhdfsstream5-1.txt
  -rw-r--r--   3 jvalkealahti supergroup         24 2013-12-18 19:03 /xd/myhdfsstream5/myhdfsstream5-2.txt
  -rw-r--r--   3 jvalkealahti supergroup         18 2013-12-18 19:03 /xd/myhdfsstream5/myhdfsstream5-3.txt

Files can be automatically partitioned using a `partitionPath` expression. If we create a stream with `idleTimeout` and `partitionPath` with simple format `yyyy/MM/dd/HH/mm` we should see writes ending into its own files within every minute boundary.

  xd:>stream create --name myhdfsstream6 --definition "time|hdfs --idleTimeout=10000 --partitionPath=dateFormat('yyyy/MM/dd/HH/mm')" --deploy

Let a stream run for a short period of time and list files.

  xd:>hadoop fs ls --recursive true --dir /xd/myhdfsstream6
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:42 /xd/myhdfsstream6/2014
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:42 /xd/myhdfsstream6/2014/05
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:42 /xd/myhdfsstream6/2014/05/28
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:45 /xd/myhdfsstream6/2014/05/28/09
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:43 /xd/myhdfsstream6/2014/05/28/09/42
  -rw-r--r--   3 jvalkealahti supergroup        140 2014-05-28 09:43 /xd/myhdfsstream6/2014/05/28/09/42/myhdfsstream6-0.txt
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:44 /xd/myhdfsstream6/2014/05/28/09/43
  -rw-r--r--   3 jvalkealahti supergroup       1200 2014-05-28 09:44 /xd/myhdfsstream6/2014/05/28/09/43/myhdfsstream6-0.txt
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:45 /xd/myhdfsstream6/2014/05/28/09/44
  -rw-r--r--   3 jvalkealahti supergroup       1200 2014-05-28 09:45 /xd/myhdfsstream6/2014/05/28/09/44/myhdfsstream6-0.txt

Partitioning can also be based on defined lists. In a below example we simulate feeding data by using a `time` and a `transform` elements. Data passed to `hdfs` sink has a content `APP0:foobar`, `APP1:foobar`, `APP2:foobar` or `APP3:foobar`.

  xd:>stream create --name myhdfsstream7 --definition "time | transform --expression=\"'APP'+T(Math).round(T(Math).random()*3)+':foobar'\" | hdfs --idleTimeout=10000 --partitionPath=path(dateFormat('yyyy/MM/dd/HH'),list(payload.split(':')[0],{{'0TO1','APP0','APP1'},{'2TO3','APP2','APP3'}}))" --deploy

Let the stream run few seconds, destroy it and check what got written in those partitioned files.

  xd:>stream destroy --name myhdfsstream7
  Destroyed stream 'myhdfsstream7'
  xd:>hadoop fs ls --recursive true --dir /xd
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/0TO1_list
  -rw-r--r--   3 jvalkealahti supergroup        108 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/0TO1_list/myhdfsstream7-0.txt
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/2TO3_list
  -rw-r--r--   3 jvalkealahti supergroup        180 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/2TO3_list/myhdfsstream7-0.txt
  xd:>hadoop fs cat /xd/myhdfsstream7/2014/05/28/19/0TO1_list/myhdfsstream7-0.txt
  APP1:foobar
  APP1:foobar
  APP0:foobar
  APP0:foobar
  APP1:foobar

Partitioning can also be based on defined ranges. In a below example we simulate feeding data by using a `time` and a `transform` elements. Data passed to `hdfs` sink has a content ranging from `APP0` to `APP15`. We simple parse the number part and use it to do a partition with ranges `{3,5,10}`.

  xd:>stream create --name myhdfsstream8 --definition "time | transform --expression=\"'APP'+T(Math).round(T(Math).random()*15)\" | hdfs --idleTimeout=10000 --partitionPath=path(dateFormat('yyyy/MM/dd/HH'),range(T(Integer).parseInt(payload.substring(3)),{3,5,10}))" --deploy

Let the stream run few seconds, destroy it and check what got written in those partitioned files.

  xd:>stream destroy --name myhdfsstream8
  Destroyed stream 'myhdfsstream8'
  xd:>hadoop fs ls --recursive true --dir /xd
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/10_range
  -rw-r--r--   3 jvalkealahti supergroup         16 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/10_range/myhdfsstream8-0.txt
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/3_range
  -rw-r--r--   3 jvalkealahti supergroup         35 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/3_range/myhdfsstream8-0.txt
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/5_range
  -rw-r--r--   3 jvalkealahti supergroup          5 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/5_range/myhdfsstream8-0.txt
  xd:>hadoop fs cat /xd/myhdfsstream8/2014/05/28/19/3_range/myhdfsstream8-0.txt
  APP3
  APP3
  APP1
  APP0
  APP1
  xd:>hadoop fs cat /xd/myhdfsstream8/2014/05/28/19/5_range/myhdfsstream8-0.txt
  APP4
  xd:>hadoop fs cat /xd/myhdfsstream8/2014/05/28/19/10_range/myhdfsstream8-0.txt
  APP6
  APP15
  APP7

Partition using a `dateFormat` can be based on content itself. This is a good use case if old log files needs to be processed where partitioning should happen based on timestamp of a log entry. We create a fake log data with a simple date string ranging from `1970-01-10` to `1970-01-13`.

  xd:>stream create --name myhdfsstream9 --definition "time | transform --expression=\"'1970-01-'+1+T(Math).round(T(Math).random()*3)\" | hdfs --idleTimeout=10000 --partitionPath=path(dateFormat('yyyy/MM/dd/HH',payload,'yyyy-MM-DD'))" --deploy

Let the stream run few seconds, destroy it and check what got written in those partitioned files. If you see the partition paths, those are based on year 1970, not present year.

  xd:>stream destroy --name myhdfsstream9
  Destroyed stream 'myhdfsstream9'
  xd:>hadoop fs ls --recursive true --dir /xd
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/10
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/10/00
  -rw-r--r--   3 jvalkealahti supergroup         44 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/10/00/myhdfsstream9-0.txt
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/11
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/11/00
  -rw-r--r--   3 jvalkealahti supergroup         99 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/11/00/myhdfsstream9-0.txt
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/12
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/12/00
  -rw-r--r--   3 jvalkealahti supergroup         44 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/12/00/myhdfsstream9-0.txt
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/13
  drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/13/00
  -rw-r--r--   3 jvalkealahti supergroup         55 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/13/00/myhdfsstream9-0.txt
  xd:>hadoop fs cat /xd/myhdfsstream9/1970/01/10/00/myhdfsstream9-0.txt
  1970-01-10
  1970-01-10
  1970-01-10
  1970-01-10

==== HDFS with Options

//^sink.hdfs
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.hdfs' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$hdfs$$** $$sink$$ has the following options:

$$closeTimeout$$:: $$timeout in ms, regardless of activity, after which file will be automatically closed$$ *($$long$$, default: `0`)*
$$codec$$:: $$compression codec alias name (gzip, snappy, bzip2, lzo, or slzo)$$ *($$String$$, default: ``)*
$$directory$$:: $$where to output the files in the Hadoop FileSystem$$ *($$String$$, default: `/xd/<stream name>`)*
$$enableSync$$:: $$whether writer will sync to datanode when flush is called, setting this to 'true' could impact throughput$$ *($$boolean$$, default: `false`)*
$$fileExtension$$:: $$the base filename extension to use for the created files$$ *($$String$$, default: `txt`)*
$$fileName$$:: $$the base filename to use for the created files$$ *($$String$$, default: `<stream name>`)*
$$fileOpenAttempts$$:: $$maximum number of file open attempts to find a path$$ *($$int$$, default: `10`)*
$$fileUuid$$:: $$whether file name should contain uuid$$ *($$boolean$$, default: `false`)*
$$flushTimeout$$:: $$timeout in ms, regardless of activity, after which data written to file will be flushed$$ *($$long$$, default: `0`)*
$$fsUri$$:: $$the URI to use to access the Hadoop FileSystem$$ *($$String$$, default: `${spring.hadoop.fsUri}`)*
$$idleTimeout$$:: $$inactivity timeout in ms after which file will be automatically closed$$ *($$long$$, default: `0`)*
$$inUsePrefix$$:: $$prefix for files currently being written$$ *($$String$$, default: ``)*
$$inUseSuffix$$:: $$suffix for files currently being written$$ *($$String$$, default: `.tmp`)*
$$overwrite$$:: $$whether writer is allowed to overwrite files in Hadoop FileSystem$$ *($$boolean$$, default: `false`)*
$$partitionPath$$:: $$a SpEL expression defining the partition path$$ *($$String$$, default: ``)*
$$rollover$$:: $$threshold in bytes when file will be automatically rolled over$$ *($$String$$, default: `1G`)*
//$sink.hdfs

NOTE: In the context of the `fileOpenAttempts` option, attempt is either one rollover request or failed stream open request for a path (if another writer came up with a same path and already opened it).

==== Partition Path Expression

SpEL expression is evaluated against a Spring Messaging `Message` passed internally into a HDFS writer. This allows expression to use `headers` and `payload` from that message. While you could do a custom processing within a stream and add custom headers, `timestamp` is always going to be there. Data to be written is then available in a `payload`.

===== Accessing Properties

Using a `payload` simply returns whatever is currently being written. Access to headers is via `headers` property. Any other property is automatically resolved from headers if found. For example `headers.timestamp` is equivalent to `timestamp`.

===== Custom Methods

Addition to a normal SpEL functionality, few custom methods has been added to make it easier to build partition paths. These custom methods can be used to work with a normal partition concepts like `date formatting`, `lists`, `ranges` and `hashes`.

====== path
[source,text]
----
path(String... paths)
----

Concatenates paths together with a delimiter `/`. This method can be used to make the expression less verbose than using a native SpEL functionality to combine path parts together. To create a path `part1/part2`, expression `'part1' + '/' + 'part2'` is equivalent to `path('part1','part2')`.

.Parameters
paths:: Any number of path parts

.Return Value
Concatenated value of paths delimited with `/`.

====== dateFormat
[source,text]
----
dateFormat(String pattern)
dateFormat(String pattern, Long epoch)
dateFormat(String pattern, Date date)
dateFormat(String pattern, String datestring)
dateFormat(String pattern, String datestring, String dateformat)
----

Creates a path using date formatting. Internally this method delegates into `SimpleDateFormat` and needs a `Date` and a `pattern`. On default if no parameter used for conversion is given, `timestamp` is expected. Effectively `dateFormat('yyyy')` equals to `dateFormat('yyyy', timestamp)` or `dateFormat('yyyy', headers.timestamp)`.

Method signature with three parameters can be used to create a custom `Date` object which is then passed to `SimpleDateFormat` conversion using a `dateformat` pattern. This is useful in use cases where partition should be based on a date or time string found from a payload content itself. Default `dateformat` pattern if omitted is `yyyy-MM-dd`.

.Parameters
pattern:: Pattern compatible with `SimpleDateFormat` to produce a final output.
epoch:: Timestamp as `Long` which is converted into a `Date`.
date:: A `Date` to be formatted.
dateformat:: Secondary pattern to convert `datestring` into a `Date`.
datestring:: `Date` as a `String`

.Return Value
A path part representation which can be a simple file or directory name or a directory structure.

====== list
[source,text]
----
list(Object source, List<List<Object>> lists)
----

Creates a partition path part by matching a `source` against a lists denoted by `lists`.

Lets assume that data is being written and it's possible to extrace an `appid` either from headers or payload. We can automatically do a list based partition by using a partition method `list(headers.appid,{{'1TO3','APP1','APP2','APP3'},{'4TO6','APP4','APP5','APP6'}})`. This method would create three partitions, `1TO3_list`, `4TO6_list` and `list`. Latter is used if no match is found from partition lists passed to `lists`.

.Parameters
source:: An `Object` to be matched against `lists`.
lists:: A definition of list of lists.

.Return Value
A path part prefixed with a matched key i.e. `XXX_list` or `list` if no match.

====== range
[source,text]
----
range(Object source, List<Object> list)
----

Creates a partition path part by matching a `source` against a list denoted by `list` using a simple binary search.

The partition method takes a `source` as first argument and `list` as a second argument. Behind the scenes this is using jvm’s `binarySearch` which works on an `Object` level so we can pass in anything. Remember that meaningful range match only works if passed in `Object` and types in list are of same type like `Integer`. Range is defined by a binarySearch itself so mostly it is to match against an upper bound except the last range in a list. Having a list of `{1000,3000,5000}` means that everything above 3000 will be matched with 5000. If that is an issue then simply adding `Integer.MAX_VALUE` as last range would overflow everything above 5000 into a new partition. Created partitions would then be `1000_range`, `3000_range` and `5000_range`.

.Parameters
source:: An `Object` to be matched against `list`.
list:: A definition of list.

.Return Value
A path part prefixed with a matched key i.e. `XXX_range`.

====== hash
[source,text]
----
hash(Object source, int bucketcount)
----

Creates a partition path part by calculating hashkey using `source`s` `hashCode` and `bucketcount`. Using a partition method `hash(timestamp,2)` would then create partitions named `0_hash`, `1_hash` and `2_hash`. Number suffixed with `_hash` is simply calculated using `Object.hashCode() % bucketcount`.

.Parameters
source:: An `Object` which `hashCode` will be used.
bucketcount:: A number of buckets

.Return Value
A path part prefixed with a hash key i.e. `XXX_hash`.

[[hdfs-dataset-avroparquet]]
=== HDFS Dataset (Avro/Parquet) (`hdfs-dataset`)

The HDFS Dataset sink is used to store Java classes that are sent as the payload on the stream. It uses the http://kitesdk.org/[Kite SDK Data Module]'s Dataset implementation to store the payload data serialized in either Avro or Parquet format. The Avro schema is generated from the Java class that is persisted. For Parquet the Java object must follow JavaBean conventions with properties for any fields to be persisted. The fields can only be simple scalar values like Strings and numbers.

The HDFS Dataset sink requires that you have a Hadoop installation that is based on Hadoop v2 (Hadoop 2.2.0, Pivotal HD 1.0, Cloudera CDH4 or Hortonworks HDP 2.0), see xref:Running-Distributed-Mode#using-hadoop[using Hadoop] for more information on how to start Spring XD to target a specific distribution.

Once Hadoop is up and running, you can then use the `hdfs-dataset` sink when creating a xref:Streams#streams[stream]

  xd:>stream create --name mydataset --definition "time | hdfs-dataset --batchSize=20" --deploy

In the above example, we've scheduled `time` source to automatically send ticks to the `hdfs-dataset` sink once every second. The data will be stored in a directory named `/xd/<streamname>` by default, so in this example it will be `/xd/mydataset`. You can change this by supplying a `--basePath` parameter and/or `--namespace` parameter. The `--basePath` defaults to `/xd` and the `--namespace` defaults to `<streamname>`. The Avro format is used by default and the data files are stored in a sub-directory named after the payload Java class. In this example the stream payload is a String so the name of the data sub-directory is `string`. If you have multiple Java classes as payloads, each class will get its own sub-directory.

Let the stream run for a minute or so. You can then list the contents of the hadoop filesystem using the shell's built in hadoop fs commands. You will first need to configure the shell to point to your name node using the hadoop config command. We use the hdfs protocol is to access the hadoop name node.

    xd:>hadoop config fs --namenode hdfs://localhost:8020

Then list the contents of the stream's data directory.

  xd:>hadoop fs ls /xd/mydataset/string
  Found 3 items
  drwxr-xr-x   - trisberg supergroup          0 2013-12-19 12:23 /xd/mydataset/string/.metadata
  -rw-r--r--   3 trisberg supergroup        202 2013-12-19 12:23 /xd/mydataset/string/1387473825754-63.avro
  -rw-r--r--   3 trisberg supergroup        216 2013-12-19 12:24 /xd/mydataset/string/1387473846708-80.avro

You can see that the sink has created two files containing the first two batches of 20 stream payloads each. There is also a `.metadata` directory created that contains the metadata that the Kite SDK Dataset implementation uses as well as the generated Avro schema for the persisted type.

  xd:>hadoop fs ls /xd/mydataset/string/.metadata
  Found 2 items
  -rw-r--r--   3 trisberg supergroup        136 2013-12-19 12:23 /xd/mydataset/string/.metadata/descriptor.properties
  -rw-r--r--   3 trisberg supergroup          8 2013-12-19 12:23 /xd/mydataset/string/.metadata/schema.avsc


Now destroy the stream.

  xd:>stream destroy --name mydataset

==== HDFS Dataset with Options

//^sink.hdfs-dataset
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.hdfs-dataset' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$hdfs-dataset$$** $$sink$$ has the following options:

$$allowNullValues$$:: $$whether null property values are allowed, if set to true then schema will use UNION for each field$$ *($$boolean$$, default: `false`)*
$$basePath$$:: $$the base directory path where the files will be written in the Hadoop FileSystem$$ *($$String$$, default: `/xd`)*
$$batchSize$$:: $$threshold in number of messages when file will be automatically flushed and rolled over$$ *($$long$$, default: `10000`)*
$$compressionType$$:: $$compression type name (snappy, deflate, bzip2 (avro only) or uncompressed)$$ *($$String$$, default: `snappy`)*
$$format$$:: $$the format to use, valid options are avro and parquet$$ *($$String$$, default: `avro`)*
$$fsUri$$:: $$the URI to use to access the Hadoop FileSystem$$ *($$String$$, default: `${spring.hadoop.fsUri}`)*
$$idleTimeout$$:: $$idle timeout in milliseconds when Hadoop file resource is automatically closed$$ *($$long$$, default: `-1`)*
$$namespace$$:: $$the sub-directory under the basePath where files will be written$$ *($$String$$, default: `<stream name>`)*
$$partitionPath$$:: $$the partition path strategy to use, a list of KiteSDK partition expressions separated by a '/' symbol$$ *($$String$$, default: ``)*
$$writerCacheSize$$:: $$the size of the cache to be used for partition writers (10 if omitted)$$ *($$int$$, default: `-1`)*
//$sink.hdfs-dataset

===== About null values
If `allowNullValues` is set to true then each field in the generated schema will use a union of 'null' and the data type of the field. You can also set `allowNullValues` to false and instead annotate fields in a POJO using Avro's `org.apache.avro.reflect.Nullable` annotation to create a schema using a union with 'null' for that annotated field.

===== About partitionPath
The `partitionPath` option lets you specify one or more paths that will be used to partition the files that the data is written to based on the content of the data. You can use any of the http://kitesdk.org/docs/0.11.0/apidocs/org/kitesdk/data/FieldPartitioner.html[FieldPartitioner]s that are available for the Kite SDK project. We simply pass in what is specified to create the corresponding partition strategy. You can separate multiple paths with a '/' character. The following partitioning functions are available:

 * _year, month, day, hour, minute_ creates partitions based on the value of a timestamp and creates directories named like "YEAR=2014" (works well with fields of datatype long)
   - specify function plus field name like: `year('timestamp')`
 * _dateformat_ creates partitions based on a timestamp and a dateformat expression provided - creates directories based on the name provided (works well with fields of datatype long)
   - specify function plus field name, a name for the partition and the date format like: `dateFormat('timestamp', 'Y-M', 'yyyyMM')`
 * _range_ creates partitions based on a field value and the upper bounds for each bucket that is specified (works well with fields of datatype int and string)
   - specify function plus field name and the upper bounds for each partition bucket like: `range('age',20,50,80,T(Integer).MAX_VALUE)` (Note that you can use SpEL expressions like we just did for the Integer.MAX_VALUE)
 * _identity_ creates partitions based on the exact value of a field (works well with fields of datatype string, long and int)
   - specify function plus field name, a name for the partition, the type of the field (String or Integer) and the number of values/buckets for the partition like: `identity('region','R',T(String),10)`
 * _hash_ creates partitions based on the hash calculated from the value of a field divided into a number of buckets that is specified (works well with all data types)
   - specify function plus field name and number of buckets like: `hash('lastname',10)`

Multiple expressions can be specified by separating them with a '/' like: `identity('region','R',T(String),10)/year('timestamp')/month('timestamp')`


[[jdbc]]
=== JDBC

The JDBC sink can be used to insert message payload data into a relational database table. By default it inserts the entire payload into a table named after the stream name in the HSQLDB database that XD uses to store metadata for batch jobs.  To alter this behavior, the jdbc sink accepts several options that you can pass using the `--foo=bar` notation in the stream, or xref:Modules#module_values[change globally]. There is also a 'config/init_db.sql' file that contains the SQL statements used to initialize the database table. You can modify this file if you'd like to create a table with your specific layout when the sink starts. You should also change the 'initializeDatabase' property to 'true' to have this script execute when the sink starts up.

The payload data will be inserted as-is if the 'names' option is set to 'payload'. This is the default behavior.  If you specify any other column names the payload data will be assumed to be a JSON document that will be converted to a hash map. This hash map will be used to populate the data values for the SQL insert statement. A matching of column names with underscores like 'user_name' will match onto camel case style keys like 'userName' in the hash map.  There will be one insert statement executed for each message.

To create a stream using a `jdbc` sink relying on all defaults you would use a command like

  xd:> stream create --name mydata --definition "time | jdbc --initializeDatabase=true" --deploy

This will insert the time messages into a 'payload' column in a table named 'mydata'. Since the default is using the XD batch metadata HSQLDB database we can connect to this database instance from an external tool. After we let the stream run for a little while, we can connect to the database and look at the data stored in the database.

You can query the database with your favorite SQL tool using the following database URL: `jdbc:hsqldb:hsql://localhost:9101/xdjob` with `sa` as the user name and a blank password. You can also use the HSQL provided SQL Tool (download from link:http://hsqldb.org/[HSQLDB]) to run a quick query from the command line:

NOTE: If you access any database other than HSQLDB or Postgres in a stream module then the JDBC driver jar for that database needs to be present in the `$XD_HOME/lib` directory.

[source,bash]
----
$ java -cp ~/Downloads/hsqldb-2.3.0/hsqldb/lib/sqltool.jar org.hsqldb.cmdline.SqlTool --inlineRc url=jdbc:hsqldb:hsql://localhost:9101/xdjob,user=sa,password= --sql "select payload from mydata;"
----

This should result in something similar to the following output:

----
2014-01-06 09:33:25
2014-01-06 09:33:26
2014-01-06 09:33:27
2014-01-06 09:33:28
2014-01-06 09:33:29
2014-01-06 09:33:30
2014-01-06 09:33:31
2014-01-06 09:33:32
2014-01-06 09:33:33
2014-01-06 09:33:34
2014-01-06 09:33:35
2014-01-06 09:33:36
2014-01-06 09:33:37
----

Now we can destroy the stream using:

  xd:> stream destroy --name mydata

==== JDBC with Options

//^sink.jdbc
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.jdbc' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$jdbc$$** $$sink$$ has the following options:

$$abandonWhenPercentageFull$$:: $$connections that have timed out wont get closed and reported up unless the number of connections in use are above the percentage$$ *($$int$$, default: `0`)*
$$alternateUsernameAllowed$$:: $$uses an alternate user name if connection fails$$ *($$boolean$$, default: `false`)*
$$columns$$:: $$the database columns to map the data to$$ *($$String$$, default: `payload`)*
$$connectionProperties$$:: $$connection properties that will be sent to our JDBC driver when establishing new connections$$ *($$String$$, no default)*
$$driverClassName$$:: $$the JDBC driver to use$$ *($$String$$, no default)*
$$fairQueue$$:: $$set to true if you wish that calls to getConnection should be treated fairly in a true FIFO fashion$$ *($$boolean$$, default: `true`)*
$$initSQL$$:: $$custom query to be run when a connection is first created$$ *($$String$$, no default)*
$$initialSize$$:: $$initial number of connections that are created when the pool is started$$ *($$int$$, default: `0`)*
$$initializeDatabase$$:: $$whether the database initialization script should be run$$ *($$boolean$$, default: `false`)*
$$initializerScript$$:: $$the name of the SQL script (in /config) to run if 'initializeDatabase' is set$$ *($$String$$, default: `init_db.sql`)*
$$jdbcInterceptors$$:: $$semicolon separated list of classnames extending org.apache.tomcat.jdbc.pool.JdbcInterceptor$$ *($$String$$, no default)*
$$jmxEnabled$$:: $$register the pool with JMX or not$$ *($$boolean$$, default: `true`)*
$$logAbandoned$$:: $$flag to log stack traces for application code which abandoned a Connection$$ *($$boolean$$, default: `false`)*
$$maxActive$$:: $$maximum number of active connections that can be allocated from this pool at the same time$$ *($$int$$, default: `100`)*
$$maxAge$$:: $$time in milliseconds to keep this connection$$ *($$int$$, default: `0`)*
$$maxIdle$$:: $$maximum number of connections that should be kept in the pool at all times$$ *($$int$$, default: `100`)*
$$maxWait$$:: $$maximum number of milliseconds that the pool will wait for a connection$$ *($$int$$, default: `30000`)*
$$minEvictableIdleTimeMillis$$:: $$minimum amount of time an object may sit idle in the pool before it is eligible for eviction$$ *($$int$$, default: `60000`)*
$$minIdle$$:: $$minimum number of established connections that should be kept in the pool at all times$$ *($$int$$, default: `10`)*
$$password$$:: $$the JDBC password$$ *($$Password$$, no default)*
$$removeAbandoned$$:: $$flag to remove abandoned connections if they exceed the removeAbandonedTimout$$ *($$boolean$$, default: `false`)*
$$removeAbandonedTimeout$$:: $$timeout in seconds before an abandoned connection can be removed$$ *($$int$$, default: `60`)*
$$suspectTimeout$$:: $$this simply logs the warning after timeout, connection remains$$ *($$int$$, default: `0`)*
$$tableName$$:: $$the database table to which the data will be written$$ *($$String$$, default: `<stream name>`)*
$$testOnBorrow$$:: $$indication of whether objects will be validated before being borrowed from the pool$$ *($$boolean$$, default: `false`)*
$$testOnReturn$$:: $$indication of whether objects will be validated before being returned to the pool$$ *($$boolean$$, default: `false`)*
$$testWhileIdle$$:: $$indication of whether objects will be validated by the idle object evictor$$ *($$boolean$$, default: `false`)*
$$timeBetweenEvictionRunsMillis$$:: $$number of milliseconds to sleep between runs of the idle connection validation/cleaner thread$$ *($$int$$, default: `5000`)*
$$url$$:: $$the JDBC URL for the database$$ *($$String$$, no default)*
$$useEquals$$:: $$true if you wish the ProxyConnection class to use String.equals$$ *($$boolean$$, default: `true`)*
$$username$$:: $$the JDBC username$$ *($$String$$, no default)*
$$validationInterval$$:: $$avoid excess validation, only run validation at most at this frequency - time in milliseconds$$ *($$long$$, default: `30000`)*
$$validationQuery$$:: $$sql query that will be used to validate connections from this pool$$ *($$String$$, no default)*
$$validatorClassName$$:: $$name of a class which implements the org.apache.tomcat.jdbc.pool.Validator$$ *($$String$$, no default)*
//$sink.jdbc

NOTE: To include the whole message into a single column, use `payload` (the default) for the `columns` option

TIP: The connection pool settings for xd are located in servers.yml (i.e. `spring.datasource.*` )



[[kafka-sink]]
=== Kafka Sink (`kafka`)

Kafka sink can be used to ingest data into a specific Kafka topic configuration.

For example,
----
xd:>stream create push-to-kafka --definition "http | kafka --topic=myTopic" --deploy
xd:>Created and deployed new stream 'push-to-kafka'
xd:>http post --data "push-messages"
> POST (text/plain;Charset=UTF-8) http://localhost:9000 push-messages
> 200 OK
----
Now, the posted messages will be available on kafka topic `myTopic`.

//^sink.kafka
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.kafka' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$kafka$$** $$sink$$ has the following options:

$$ackTimeoutOnServer$$:: $$the maximum amount of time the server will wait for acknowledgments from followers to meet the acknowledgment requirements the producer has specified with the acks configuration$$ *($$int$$, default: `30000`)*
$$batchBytes$$:: $$batch size in bytes, per partition$$ *($$int$$, default: `16384`)*
$$blockOnBufferFull$$:: $$whether to block or not when the memory buffer is full$$ *($$boolean$$, default: `true`)*
$$brokerList$$:: $$comma separated broker list$$ *($$String$$, default: `localhost:9092`)*
$$bufferMemory$$:: $$the total bytes of memory the producer can use to buffer records waiting to be sent to the server$$ *($$int$$, default: `33554432`)*
$$compressionCodec$$:: $$compression codec to use$$ *($$String$$, default: `none`)*
$$maxBufferTime$$:: $$the amount of time, in ms that the producer will wait before sending a batch to the server$$ *($$int$$, default: `0`)*
$$maxRequestSize$$:: $$the maximum size of a request$$ *($$int$$, default: `1048576`)*
$$maxSendRetries$$:: $$number of attempts to automatically retry a failed send request$$ *($$int$$, default: `3`)*
$$receiveBufferBytes$$:: $$the size of the TCP receive buffer to use when reading data$$ *($$int$$, default: `32768`)*
$$reconnectBackoff$$:: $$the amount of time to wait before attempting to reconnect to a given host when a connection fails$$ *($$long$$, default: `10`)*
$$requestRequiredAck$$:: $$producer request acknowledgement mode$$ *($$int$$, default: `0`)*
$$retryBackoff$$:: $$the amount of time to wait before attempting to retry a failed produce request to a given topic partition$$ *($$long$$, default: `100`)*
$$sendBufferBytes$$:: $$the size of the TCP send buffer to use when sending data$$ *($$int$$, default: `131072`)*
$$topic$$:: $$kafka topic name$$ *($$String$$, default: `<stream name>`)*
$$topicMetadataFetchTimeout$$:: $$the maximum amount of time to block waiting for the metadata fetch to succeed$$ *($$int$$, default: `60000`)*
$$topicMetadataRefreshInterval$$:: $$the period of time in milliseconds after which a refresh of metadata is forced$$ *($$int$$, default: `300000`)*
//$sink.kafka

[[log]]
=== Log

Probably the simplest option for a sink is just to log the data. The `log` sink uses the application logger to output the data for inspection. The log level is set to `WARN` and the logger name is created from the stream name. To create a stream using a `log` sink you would use a command like

  xd:> stream create --name mylogstream --definition "http --port=8000 | log" --deploy

You can then try adding some data. We've used the `http` source on port 8000 here, so run the following command to send a message

  xd:> http post --target http://localhost:8000 --data "hello"

and you should see the following output in the XD container console.

  13/06/07 16:12:18 INFO sink.mylogstream: hello

//^sink.log
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.log' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$log$$** $$sink$$ has the following options:

$$expression$$:: $$the expression to be evaluated for the log content; use '#root' to log the full message$$ *($$String$$, default: `payload`)*
$$level$$:: $$the log level$$ *($$String$$, default: `INFO`)*
$$name$$:: $$the name of the log category to log to (will be prefixed by 'xd.sink.')$$ *($$String$$, default: `<stream name>`)*
//$sink.log

Here are some examples explaining the above options:

The logger name is the sink name prefixed with the string `xd.sink.`. The sink name is the same as the stream name by default, but you can set it by passing the `--name` parameter

  xd:> stream create --name myotherlogstream --definition "http --port=8001 | log --name=mylogger" --deploy

The log level is `INFO` by default; this can be changed with the `--level` property (`FATAL`, `ERROR`, `WARN`, `INFO`, `DEBUG`, or `TRACE`)

  xd:> stream create --name mylogstream --definition "http --port=8001 | log --level=WARN" --deploy

By default, the message payload is logged; this can be changed with the `--expression` property (e.g. `payload.foo` to log some property `foo` of the payload, or `#root` to log the entire message)

  xd:> stream create --name mylogstream --definition "http --port=8001 | log --expression=#root" --deploy

[[mail]]
=== Mail

The "mail" sink allows sending of messages as emails, leveraging Spring Integration mail-sending channel adapter. Please refer to Spring Integration documentation for the details, but in a nutshell, the sink is able to handle String, byte[] and MimeMessage messages out of the box.

Here is a simple example of how the mail module is used:

  xd:> stream create mystream --definition "http | mail --to='\"your.email@gmail.com\"' --host=your.imap.server --subject=payload+' world'" --deploy

Then,

  xd:> http post --data Hello

You would then receive an email whose body contains "Hello" and whose subject is "Hellow world". Of special attention here is the way you need to escape strings for most of the parameters, because they're actually SpEL expressions (so here for example, we used a String literal for the `to` parameter).

//^sink.mail
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.mail' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$mail$$** $$sink$$ has the following options:

$$bcc$$:: $$the recipient(s) that should receive a blind carbon copy (SpEL)$$ *($$String$$, default: `null`)*
$$cc$$:: $$the recipient(s) that should receive a carbon copy (SpEL)$$ *($$String$$, default: `null`)*
$$contentType$$:: $$the content type to use when sending the email (SpEL)$$ *($$String$$, default: `null`)*
$$from$$:: $$the primary recipient(s) of the email (SpEL)$$ *($$String$$, default: `null`)*
$$host$$:: $$the hostname of the mail server$$ *($$String$$, default: `localhost`)*
$$password$$:: $$the password to use to connect to the mail server $$ *($$String$$, no default)*
$$port$$:: $$the port of the mail server$$ *($$int$$, default: `25`)*
$$replyTo$$:: $$the address that will become the recipient if the original recipient decides to "reply to" the email (SpEL)$$ *($$String$$, default: `null`)*
$$subject$$:: $$the email subject (SpEL)$$ *($$String$$, default: `null`)*
$$to$$:: $$the primary recipient(s) of the email (SpEL)$$ *($$String$$, default: `null`)*
$$username$$:: $$the username to use to connect to the mail server$$ *($$String$$, no default)*
//$sink.mail

[[mongo]]
=== Mongo
The Mongo sink writes into a Mongo collection. Here is a simple example

  xd:>stream create --name attendees --definition "http | mongodb --databaseName=test --collectionName=names" --deploy

Then,

  xd:>http post --data {"firstName":"mark"}

In the mongo console you will see the document stored

  > use test
  switched to db test
  > show collections
  names
  system.indexes
  > db.names.find()
  { "_id" : ObjectId("53c93bc324ac76925a77b9df"), "firstName" : "mark" }

//^sink.mongodb
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.mongodb' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$mongodb$$** $$sink$$ has the following options:

$$authenticationDatabaseName$$:: $$the MongoDB authentication database used for connecting$$ *($$String$$, default: ``)*
$$collectionName$$:: $$the MongoDB collection to store$$ *($$String$$, default: `<stream name>`)*
$$databaseName$$:: $$the MongoDB database name$$ *($$String$$, default: `xd`)*
$$host$$:: $$the MongoDB host to connect to$$ *($$String$$, default: `localhost`)*
$$password$$:: $$the MongoDB password used for connecting$$ *($$String$$, default: ``)*
$$port$$:: $$the MongoDB port to connect to$$ *($$int$$, default: `27017`)*
$$username$$:: $$the MongoDB username used for connecting$$ *($$String$$, default: ``)*
$$writeConcern$$:: $$the default MongoDB write concern to use$$ *($$WriteConcern$$, default: `SAFE`, possible values: `NONE,NORMAL,SAFE,FSYNC_SAFE,REPLICAS_SAFE,JOURNAL_SAFE,MAJORITY`)*
//$sink.mongodb

[[mqtt-sink]]
=== MQTT Sink (`mqtt`)
The mqtt sink connects to an mqtt server and publishes telemetry messages.

==== Options
//^sink.mqtt
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.mqtt' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$mqtt$$** $$sink$$ has the following options:

$$async$$:: $$whether or not to use async sends$$ *($$boolean$$, default: `false`)*
$$charset$$:: $$the charset used to convert a String payload to byte[]$$ *($$String$$, default: `UTF-8`)*
$$cleanSession$$:: $$whether the client and server should remember state across restarts and reconnects$$ *($$boolean$$, default: `true`)*
$$clientId$$:: $$identifies the client$$ *($$String$$, default: `xd.mqtt.client.id.snk`)*
$$connectionTimeout$$:: $$the connection timeout in seconds$$ *($$int$$, default: `30`)*
$$keepAliveInterval$$:: $$the ping interval in seconds$$ *($$int$$, default: `60`)*
$$password$$:: $$the password to use when connecting to the broker$$ *($$String$$, default: `guest`)*
$$persistence$$:: $$'memory' or 'file'$$ *($$String$$, default: `memory`)*
$$persistenceDirectory$$:: $$file location when using 'file' persistence$$ *($$String$$, default: `/tmp/paho`)*
$$qos$$:: $$the quality of service to use$$ *($$int$$, default: `1`)*
$$retained$$:: $$whether to set the 'retained' flag$$ *($$boolean$$, default: `false`)*
$$topic$$:: $$the topic to which the sink will publish$$ *($$String$$, default: `xd.mqtt.test`)*
$$url$$:: $$location of the mqtt broker(s) (comma-delimited list)$$ *($$String$$, default: `tcp://localhost:1883`)*
$$username$$:: $$the username to use when connecting to the broker$$ *($$String$$, default: `guest`)*
//$sink.mqtt

NOTE: The defaults are set up to connect to the RabbitMQ MQTT adapter on localhost.

[[null-sink]]
=== Null Sink (`null`)

Null sink can be useful when the main stream isn't focused on a specific destination but drives taps used for analytics etc.
It is also useful to iteratively add in steps to a stream without worrying about where the data will end up.

For example,

----
xd:>stream create nullStream --definition "http | null" --deploy
Created and deployed new stream 'nullStream'
xd:>stream create tap1 --definition "tap:stream:nullStream > counter" --deploy
Created and deployed new stream 'tap1'
----
In the above, the null sink can be useful as we can create as many number of tap streams off the main stream while we set the main stream sink as null.

[[rabbitmq]]
=== RabbitMQ

The "rabbit" sink enables outbound messaging over RabbitMQ.

The following example shows the default settings.

Configure a stream:

     xd:> stream create --name rabbittest --definition "time --interval=3 | rabbit" --deploy

This sends the time, every 3 seconds to the default (no-name) Exchange for a RabbitMQ broker running on localhost, port 5672.

The routing key will be the name of the stream by default; in this case: "rabbittest". Since the default Exchange is a direct-exchange to which all Queues are bound with the Queue name as the binding key, all messages sent via this sink will be passed to a Queue named "rabbittest", if one exists. We do not create that Queue automatically. However, you can easily create a Queue using the RabbitMQ web UI. Then, using that same UI, you can navigate to the "rabbittest" Queue and click the "Get Message(s)" button to pop messages off of that Queue (you can choose whether to requeue those messages).

To destroy the stream, enter the following at the shell prompt:

    xd:> stream destroy --name rabbittest

==== RabbitMQ with Options

//^sink.rabbit
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.rabbit' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$rabbit$$** $$sink$$ has the following options:

$$addresses$$:: $$a comma separated list of 'host[:port]' addresses$$ *($$String$$, default: `${spring.rabbitmq.addresses}`)*
$$channelCacheSize$$:: $$the channel cache size$$ *($$String$$, default: `${spring.rabbitmq.channelCacheSize}`)*
$$converterClass$$:: $$the class name of the message converter$$ *($$String$$, default: `org.springframework.amqp.support.converter.SimpleMessageConverter`)*
$$deliveryMode$$:: $$the delivery mode (PERSISTENT, NON_PERSISTENT)$$ *($$String$$, default: `PERSISTENT`)*
$$exchange$$:: $$the Exchange on the RabbitMQ broker to which messages should be sent$$ *($$String$$, default: ``)*
$$keyStore$$:: $$keyStore location (if not using SSL properties)$$ *($$String$$, default: `${spring.rabbitmq.ssl.keyStore}`)*
$$keyStorePassphrase$$:: $$keyStore passphrase (if not using SSL properties)$$ *($$String$$, default: `${spring.rabbitmq.ssl.keyStorePassphrase}`)*
$$mappedRequestHeaders$$:: $$request message header names to be propagated to/from the adpater/gateway$$ *($$String$$, default: `STANDARD_REQUEST_HEADERS`)*
$$password$$:: $$the password to use to connect to the broker$$ *($$String$$, default: `${spring.rabbitmq.password}`)*
$$routingKey$$:: $$the routing key to be passed with the message, as a SpEL expression$$ *($$String$$, default: `'<stream name>'`)*
$$sslPropertiesLocation$$:: $$resource containing SSL properties$$ *($$String$$, default: `${spring.rabbitmq.sslProperties}`)*
$$trustStore$$:: $$trustStore location (if not using SSL properties)$$ *($$String$$, default: `${spring.rabbitmq.ssl.trustStore}`)*
$$trustStorePassphrase$$:: $$trustStore passphrase (if not using SSL properties)$$ *($$String$$, default: `${spring.rabbitmq.ssl.trustStorePassphrase}`)*
$$useSSL$$:: $$true if SSL should be used for the connection$$ *($$String$$, default: `${spring.rabbitmq.useSSL}`)*
$$username$$:: $$the username to use to connect to the broker$$ *($$String$$, default: `${spring.rabbitmq.username}`)*
$$vhost$$:: $$the RabbitMQ virtual host to use$$ *($$String$$, default: `${spring.rabbitmq.virtual_host}`)*
//$sink.rabbit

[NOTE]
====
Please be aware that the `routingKey` option is actually a SpEL expression. Hence if a simple, constant, string literal is to be used, make sure to use something like this:

   xd:> stream create rabbitSinkStream --definition "http | rabbit --routingKey='\"myqueue\"'" --deploy
====

See the xref:MessageBus#rabbitssl[RabbitMQ MessageBus Documentation] for more information about SSL configuration.

[[redis]]
=== Redis

Redis sink can be used to ingest data into redis store. You can choose `queue`, `topic` or `key` with selcted collection type to point to a specific data store.

For example,
----
xd:>stream create store-into-redis --definition "http | redis --queue=myList" --deploy
xd:>Created and deployed new stream 'store-into-redis'
----

==== Options

//^sink.redis
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.redis' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$redis$$** $$sink$$ has the following options:

$$collectionType$$:: $$the collection type to use for the given key$$ *($$CollectionType$$, default: `LIST`, possible values: `LIST,SET,ZSET,MAP,PROPERTIES`)*
$$database$$:: $$database index used by the connection factory$$ *($$int$$, default: `0`)*
$$hostname$$:: $$redis host name$$ *($$String$$, default: `localhost`)*
$$key$$:: $$name for the key$$ *($$String$$, no default)*
$$keyExpression$$:: $$a SpEL expression to use for keyExpression$$ *($$String$$, no default)*
$$maxActive$$:: $$max number of connections that can be allocated by the pool at a given time; negative value for no limit$$ *($$int$$, default: `8`)*
$$maxIdle$$:: $$max number of idle connections in the pool; a negative value indicates an unlimited number of idle connections$$ *($$int$$, default: `8`)*
$$maxWait$$:: $$max amount of time (in milliseconds) a connection allocation should block before throwing an exception when the pool is exhausted; negative value to block indefinitely$$ *($$int$$, default: `-1`)*
$$minIdle$$:: $$target for the minimum number of idle connections to maintain in the pool; only has an effect if it is positive$$ *($$int$$, default: `0`)*
$$password$$:: $$redis password$$ *($$String$$, default: ``)*
$$port$$:: $$redis port$$ *($$int$$, default: `6379`)*
$$queue$$:: $$name for the queue$$ *($$String$$, no default)*
$$queueExpression$$:: $$a SpEL expression to use for queue$$ *($$String$$, no default)*
$$sentinelMaster$$:: $$name of Redis master server$$ *($$String$$, default: ``)*
$$sentinelNodes$$:: $$comma-separated list of host:port pairs$$ *($$String$$, default: ``)*
$$topic$$:: $$name for the topic$$ *($$String$$, no default)*
$$topicExpression$$:: $$a SpEL expression to use for topic$$ *($$String$$, no default)*
//$sink.redis


[[shell-sink]]
=== Shell Sink (`shell`)
The `shell` sink forks an external process by running a shell command to launch a process written in any language. The process should implement a continual loop that waits for and consumes input from `stdin`. The process will be destroyed when the stream is undeployed. For example, it is possible to invoke a Python script within a stream in this manner. Since the shell sink relies on low-level stream processing there are some additional requirements:

* Input data is expected to be a String, the `charset` is configurable.
* Anything written to `stderr` will be logged as an ERROR in Spring XD but will not terminate the stream.
* All messages must be terminated using the configured encoder (CRLF or "\r\n" is the default) for the module and must not exceed the configured `bufferSize` (see the detailed description of encoders in the <<tcp-sink, TCP>> section).
* Any external software required to run the script must be installed on the container node to which the module is deployed.

Here is a simple template for a Python script that consumes input:

[source,python]
----
#sink.py
import sys

while True:
  try:
    data = raw_input()
    if data:
       #insert a function call here, data is a string.
  except EOFError:
      break
----

[NOTE]
====
Spring XD provides additional Python programming support for handling basic stream processing, as shown above, see xref:Creating-a-Python-Module[creating a Python module].
====

//^sink.shell
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.shell' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$shell$$** $$sink$$ has the following options:

$$bufferSize$$:: $$the size of the buffer (bytes) to use when encoding/decoding$$ *($$int$$, default: `2048`)*
$$charset$$:: $$the charset used when converting from String to bytes$$ *($$String$$, default: `UTF-8`)*
$$command$$:: $$the shell command$$ *($$String$$, no default)*
$$encoder$$:: $$the encoder to use when sending messages$$ *($$Encoding$$, default: `CRLF`, possible values: `CRLF,LF,NULL,STXETX,RAW,L1,L2,L4`)*
$$environment$$:: $$additional process environment variables as comma delimited name-value pairs$$ *($$String$$, no default)*
$$redirectErrorStream$$:: $$redirects stderr to stdout$$ *($$boolean$$, default: `false`)*
$$workingDir$$:: $$the process working directory$$ *($$String$$, no default)*
//$sink.shell

[[splunk-server]]
=== Splunk Server (`splunk`)
A https://www.splunk.com/[Splunk] sink that writes data to a TCP Data Input type for Splunk.

==== Splunk sinks
The Splunk sink converts an object payload to a string using the object’s toString method and then converts this to a SplunkEvent that is sent via TCP to Splunk.

//^sink.splunk
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.splunk' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$splunk$$** $$sink$$ has the following options:

$$host$$:: $$the host name or IP address of the Splunk server$$ *($$String$$, default: `localhost`)*
$$owner$$:: $$the owner of the tcpPort$$ *($$String$$, default: `admin`)*
$$password$$:: $$the password associated with the username$$ *($$String$$, default: `password`)*
$$port$$:: $$the TCP port number of the Splunk server$$ *($$int$$, default: `8089`)*
$$tcpPort$$:: $$the TCP port number to where XD will send the data$$ *($$int$$, default: `9500`)*
$$username$$:: $$the login name that has rights to send data to the tcpPort$$ *($$String$$, default: `admin`)*
//$sink.splunk

==== How To Setup Splunk for TCP Input
. From the Manager page select `Manage Inputs` link
. Click the `Add data` Button
. Click the `From a TCP port` link
. `TCP Port` enter the port you want Splunk to monitor
. `Set Source Type` select `Manual`
. `Source Type` enter `tcp-raw`
. Click `Save`

==== Example
An example stream would be to take data from a twitter search and push it through to a splunk instance.

    xd:> stream create --name springone2gx --definition "twittersearch --consumerKey= --consumerSecret= --query='#LOTR' | splunk" --deploy

[[tcp-sink]]
=== TCP Sink (`tcp`)

The TCP Sink provides for outbound messaging over TCP.

The following examples use `netcat` (linux) to receive the data; the equivalent on Mac OSX is `nc`.

First, start a netcat to receive the data, and background it

[source,bash]
----
$ netcat -l 1234 &
----

Now, configure a stream

     xd:> stream create --name tcptest --definition "time --fixedDelay=3 | tcp" --deploy

This sends the time, every 3 seconds to the default tcp Sink, which connects to port `1234` on `localhost`.

----
$ Thu May 30 10:28:21 EDT 2013
Thu May 30 10:28:24 EDT 2013
Thu May 30 10:28:27 EDT 2013
Thu May 30 10:28:30 EDT 2013
Thu May 30 10:28:33 EDT 2013
----

TCP is a streaming protocol and some mechanism is needed to frame messages on the wire. A number of encoders are available, the default being 'CRLF'.

Destroy the stream; netcat will terminate when the TCP Sink disconnects.

    http://localhost:8080> stream destroy --name tcptest

==== TCP with Options

//^sink.tcp
// DO NOT MODIFY THE LINES BELOW UNTIL THE CLOSING '//$sink.tcp' TAG
// THIS SNIPPET HAS BEEN GENERATED BY ModuleOptionsReferenceDoc AND MANUAL EDITS WILL BE LOST
The **$$tcp$$** $$sink$$ has the following options:

$$bufferSize$$:: $$the size of the buffer (bytes) to use when encoding/decoding$$ *($$int$$, default: `2048`)*
$$charset$$:: $$the charset used when converting from String to bytes$$ *($$String$$, default: `UTF-8`)*
$$close$$:: $$whether to close the socket after each message$$ *($$boolean$$, default: `false`)*
$$encoder$$:: $$the encoder to use when sending messages$$ *($$Encoding$$, default: `CRLF`, possible values: `CRLF,LF,NULL,STXETX,RAW,L1,L2,L4`)*
$$host$$:: $$the remote host to connect to$$ *($$String$$, default: `localhost`)*
$$nio$$:: $$whether or not to use NIO$$ *($$boolean$$, default: `false`)*
$$port$$:: $$the port on the remote host to connect to$$ *($$int$$, default: `1234`)*
$$reverseLookup$$:: $$perform a reverse DNS lookup on the remote IP Address$$ *($$boolean$$, default: `false`)*
$$socketTimeout$$:: $$the timeout (ms) before closing the socket when no data is received$$ *($$int$$, default: `120000`)*
$$useDirectBuffers$$:: $$whether or not to use direct buffers$$ *($$boolean$$, default: `false`)*
//$sink.tcp

NOTE: With the default retry configuration, the attempts will be made after 0, 2, 4, 8, and 16 seconds.

==== Available Encoders

.Text Data

CRLF (default):: text terminated by carriage return (0x0d) followed by line feed (0x0a)
LF:: text terminated by line feed (0x0a)
NULL:: text terminated by a null byte (0x00)
STXETX:: text preceded by an STX (0x02) and terminated by an ETX (0x03)

.Text and Binary Data

RAW:: no structure - the client indicates a complete message by closing the socket
L1:: data preceded by a one byte (unsigned) length field (supports up to 255 bytes)
L2:: data preceded by a two byte (unsigned) length field (up to 2^16^-1 bytes)
L4:: data preceded by a four byte (signed) length field (up to 2^31^-1 bytes)


==== An Additional Example

Start netcat in the background and redirect the output to a file `foo`

[source,bash]
----
$ netcat -l 1235 > foo &
----

Create the stream, using the `L4` encoder

     xd:> stream create --name tcptest --definition "time --interval=3 | tcp --encoder=L4 --port=1235" --deploy

Destroy the stream

     http://localhost:8080> stream destroy --name tcptest

Check the output

[source,bash]
----
$ hexdump -C foo
00000000  00 00 00 1c 54 68 75 20  4d 61 79 20 33 30 20 31  |....Thu May 30 1|
00000010  30 3a 34 37 3a 30 33 20  45 44 54 20 32 30 31 33  |0:47:03 EDT 2013|
00000020  00 00 00 1c 54 68 75 20  4d 61 79 20 33 30 20 31  |....Thu May 30 1|
00000030  30 3a 34 37 3a 30 36 20  45 44 54 20 32 30 31 33  |0:47:06 EDT 2013|
00000040  00 00 00 1c 54 68 75 20  4d 61 79 20 33 30 20 31  |....Thu May 30 1|
00000050  30 3a 34 37 3a 30 39 20  45 44 54 20 32 30 31 33  |0:47:09 EDT 2013|
----

Note the 4 byte length field preceding the data generated by the `L4` encoder.

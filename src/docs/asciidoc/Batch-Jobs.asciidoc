[[batch]]
== Batch Jobs

=== Introduction

One of the features that XD offers is the ability to launch and monitor batch jobs based on https://www.springsource.org/spring-batch[Spring Batch].  The Spring Batch project was started in 2007 as a collaboration between SpringSource and Accenture to provide a comprehensive framework to support the development of robust batch applications.  Batch jobs have their own set of best practices and domain concepts which have been incorporated into Spring Batch building upon Accenture's consulting business.  Since then Spring Batch has been used in thousands of enterprise applications and is the basis for the recent JSR standardization of batch processing, https://jcp.org/en/jsr/detail?id=352[JSR-352].

Spring XD builds upon Spring Batch to simplify creating batch workflow solutions that span traditional use-cases such as moving data between flat files and relational databases as well as Hadoop use-cases where analysis logic is broken up into several steps that run on a Hadoop cluster.  Steps specific to Hadoop in a workflow can be MapReduce jobs, executing Hive/Pig scripts or HDFS operations. Spring XD ships with a number of useful xref:Jobs#jobs[job modules out of the box]. This section presents an overview for creating, deploying, and running batch jobs.

=== Workflow

The concept of a workflow translates to a Job, not to be confused with a MapReduce job. A Job is a directed graph, each node of the graph is a processing Step. Steps can be executed sequentially or in parallel, depending on the configuration. Jobs can be started, stopped, and restarted. Restarting
jobs is possible since the progress of executed steps in a Job is persisted in a database via a JobRepository.  The following figures shows the basic components of a workflow.

image::images/batch-overview.png[The Spring XD Workflow overview, width=500]

A Job that has steps specific to Hadoop is shown below.

image::images/batch-hadoop-overview.png[Steps in a workflow that execute Hadoop HDFS operations and run Pig, MapReduce and Hive jobs, width=200]

A JobLauncher is responsible for starting a job and is often triggered via a scheduler.  Other options to launch a job are through Spring XD's RESTful administration API, the XD web application, or in response to an external event from and XD stream definition, _e.g._ file polling using the file source.

=== Features

Spring XD allows you to create and launch jobs.  The launching of a job can be triggered using a cron expression or in reaction to data on a stream. When jobs are executing, they are also a source of event data that can be subscribed to by a stream.  There are several type of events sent during a job's execution, the most common being the status of the job and the steps taken within the job.  This bi-direction communication between stream processing and batch processing allows for more complex chains of processing to be developed.

As a starting point, jobs for the following cases are provided to use out of the box

* Poll a Directory and import CSV files to HDFS
* Import CSV files to JDBC
* HDFS to JDBC Export
* JDBC to HDFS Import
* HDFS to MongoDB Export

These are described in the section below.

The purpose of this section is to show you how to create, schedule and monitor a job.

=== The Lifecycle of a Job in Spring XD

Before we dive deeper into the details of creating batch jobs with Spring XD, we need to understand the typical lifecycle for batch jobs in the context of _Spring XD_:

 . Register a Job Module
 . Create a Job Definition
 . Deploy a Job
 . Launch a Job
 . Job Execution
 . Un-deploy a Job
 . Destroy a Job Definition

==== Register a Job Module

Register a *Job Module* with the _Module Registry_ using the XD Shell `module upload` command. See xref:Modules#registering-a-module[registering a module] for details.

==== Create a Job Definition

Create a *Job Definition* from a _Job Module_ by providing a definition name as well as properties that apply to all _Job Instances_. At this point the job is not deployed, yet.

==== Deploy the Job

Deploy the *Job Definition* to one or more _Spring XD_ containers. This will initialize the _Job Definitions_ on those containers. The jobs are now "live" and a job can be created by sending a message to a job queue that contains optional runtime *https://docs.spring.io/spring-batch/trunk/reference/html/domain.html#domainJobParameters[Job Parameters]*.

==== Launch a Job

Launch a job by sending a message to the job queue with *https://docs.spring.io/spring-batch/trunk/reference/html/domain.html#domainJobParameters[Job Parameters]*. A *https://docs.spring.io/spring-batch/trunk/reference/html/domain.html#domainJobInstance[Job Instance]* is created, representing a specific run of the job. A *Job Instance* is the *Job Definition* plus the runtime *Job Parameters*. You can query for the *Job Instances* associated with a given job name.

==== Job Execution

The job is executed creating a *https://docs.spring.io/spring-batch/trunk/reference/html/domain.html#domainJobExecution[Job Execution]* object that captures the success or failure of the job. You can query for *Job Executions* associated with a given job name.

==== Un-deploy a Job

This removes the job from the _Spring XD_ container(s) preventing the launching of any new _Job Instances_. For reporting purposes, you will still be able to view historic _Job Executions_ associated with the the job.

==== Destroy a Job Definition

Destroying a *Job Definition* will not only un-deploy any still deployed _Job Definitions_ but will also remove the _Job Definition_ itself.

[[job_options]]
==== Creating Jobs - Additional Options

When creating jobs, the following options are available to all job definitions:

dateFormat:: The optional date format for job parameters *(default: `yyyy-MM-dd`)*
numberFormat:: Defines the number format when parsing numeric parameters *(default: `NumberFormat.getInstance(Locale.US)`)*
makeUnique:: Shall job parameters be made unique? *(default: `true`)*

Also, similar to the `stream create` command, the `job create` command has an optional `--deploy` option to create the job definition and deploy it. `--deploy` option is false by default.

Below is an example of some of these options combined:

----
job create myjob --definition "fooJob --makeUnique=false"
----

Remember that you can always find out about available options for a job by using the xref:Modules.asciidoc#module_info[`module info`] command.

=== Deployment manifest support for job

When deploying batch job you can provide a xref:Deployment#deployment-manifest[deployment manifest]. Deployment manifest properties for jobs are the same as for streams, you can declare

* The number of job modules to deploy
* The criteria expression to use for matching the job to available containers

For example,

----
job create myjob --definition "fooJob --makeUnique=false"

job deploy myjob --properties "module.fooJob.count=3,module.fooJob.criteria=groups.contains('hdfs-containers-group')"
----

The above deployment manifest would deploy 3 number of `fooJob` modules into containers whose group name matches "hdfs-containers-group".

When a batch job is launched/scheduled, the job module that picks up the job launching request message executes the batch job.  To support partitioning of the job across multiple containers, the job definition needs to define how the job will be partitioned.  The type of partitioning depends on the type of the job, for example a job reading from JDBC would partition the data in a table by dividing up the number of rows and a job reading files form a directory would partition on the number of files available.

The FTP to HDFS and FILE to JDBC jobs support for partitioning.  To add partitioning support for your own jobs you should import https://github.com/spring-projects/spring-xd/blob/master/spring-xd-dirt/src/main/resources/META-INF/spring-xd/batch/singlestep-partition-support.xml[singlestep-partition-support.xml] in your job definition.  This provides the infrastructure so that the job module that processes the launch request can communicate as the master with the other job modules that have been deployed.  You will also need to provide an implementation of the https://docs.spring.io/spring-batch/apidocs/org/springframework/batch/core/partition/support/Partitioner.html[Partitioner] interface.

For more information on the deployment manifest, please refer https://github.com/spring-projects/spring-xd/wiki/XD-Distributed-Runtime#deployment-manifest[here]

=== Launching a job
XD uses triggers as well as regular event flow to launch the batch jobs.  So in this section we will cover how to:

* Launch the Batch Job Ad-hoc
* Launch the Batch Job using a named Cron-Trigger
* Launch the Batch Job as sink.

==== Ad-hoc
To launch a job one time, use the launch option of the job command.  So going back to our example above, we've created a job module instance named helloSpringXD.  Launching that Job Module Instance would look like:
----
xd:> job launch helloSpringXD
----
In the logging output of the XDContainer you should see the following
----
16:45:40,127  INFO http-bio-9393-exec-1 job.JobPlugin:98 - Configuring module with the following properties: {numberFormat=, dateFormat=, makeUnique=true, xd.job.name=myjob}
16:45:40,185  INFO http-bio-9393-exec-1 module.SimpleModule:140 - initialized module: SimpleModule [name=job, type=job, group=myjob, index=0 @3a9ecb9d]
16:45:40,198  INFO http-bio-9393-exec-1 module.SimpleModule:161 - started module: SimpleModule [name=job, type=job, group=myjob, index=0 @3a9ecb9d]
16:45:40,199  INFO http-bio-9393-exec-1 module.ModuleDeployer:161 - deployed SimpleModule [name=job, type=job, group=myjob, index=0 @3a9ecb9d]
Hello Spring XD!
----
To re-launch the job just execute the launch command.
For example:
----
xd:> job launch helloSpringXD
----
==== Launch the Batch using Cron-Trigger
To launch a batch job based on a cron scheduler is done by creating a stream using the trigger source.

----
xd:> stream create --name cronStream --definition "trigger --cron='0/5 * * * * *'  > queue:job:myCronJob" --deploy

----
A batch job can receive parameters from a source (in this case a trigger) or process. A trigger uses the --payload expression to declare its payload.
----
xd:> stream create --name cronStream --definition "trigger --cron='0/5 * * * * *'  --payload={\"param1\":\"Kenny\"} > queue:job:myCronJob" --deploy
----
NOTE: The payload content must be in a JSON-based map representation.

To pause/stop future scheduled jobs from running for this stream, the stream must be undeployed for example:
----
xd:> stream undeploy --name cronStream
----
==== Launch the Batch using a Fixed-Delay-Trigger
A fixed-delay-trigger is used to launch a Job on a regular interval.  Using the --fixedDelay parameter you can set up the number of seconds between executions.  In the example below we are running myXDJob every 10 seconds and passing it a payload containing a single attribute.
----
xd:> stream create --name fdStream --definition "trigger --payload={\"param1\":\"fixedDelayKenny\"} --fixedDelay=5 > queue:job:myXDJob" --deploy
----
To pause/stop future scheduled jobs from running for this stream, you must undeploy the stream for example:
----
xd:> stream undeploy --name fdStream
----
==== Launch job as a part of event flow
A batch job is always used as a sink, with that being said it can receive messages from sources (other than triggers) and processors. In the case below we see that the user has created an http source (http source receives http posts and passes the payload of the http message to the next module in the stream) that will pass the http payload to the "myHttpJob".

----
 stream create --name jobStream --definition "http > queue:job:myHttpJob" --deploy
----
To test the stream you can execute a http post, like the following:
----
xd:> http post --target http://localhost:9000 --data "{\"param1\":\"fixedDelayKenny\"}"
----
=== Retrieve job notifications

Spring XD offers the facilities to capture the notifications that are sent from the job as it is executing.
When a batch job is deployed, by default it registers the following listeners along with pub/sub channels that these listeners send messages to.

* Job Execution Listener
* Chunk Listener
* Item Listener
* Step Execution Listener
* Skip Listener

Along with the pub/sub channels for each of these listeners, there will also be a pub/sub channel that the aggregated events from all these listeners are published to.

In the following example, we setup a Batch Job called _myHttpJob_. Afterwards we create a stream that will tap into the pub/sub channels that were implicitly generated when the _myHttpJob_ job was deployed.

==== To receive aggregated events

The stream receives aggregated event messages from all the default batch job listeners and sends those messages to the log.
----
xd>job create --name myHttpJob --definition "httpJob" --deploy
xd>stream create --name aggregatedEvents --definition "tap:job:myHttpJob >log" --deploy
xd>job launch myHttpJob
----

**Note:** The syntax for the tap that receives the aggregated events is: `tap:job:<job-name>`


In the logging output of the container you should see something like the following when the job completes (with the aggregated events
----
09:55:53,532  WARN SimpleAsyncTaskExecutor-1 logger.aggregatedEvents:150 - JobExecution: id=2, version=1, startTime=Sat Apr 12 09:55:53 PDT 2014, endTime=null, lastUpdated=Sat Apr 12 09:55:53 PDT 2014, status=STARTED, exitStatus=exitCode=UNKNOWN;exitDescription=, job=[JobInstance: id=2, version=0, Job=[myHttpJob]], jobParameters=[{random=0.07002785662707867}]
09:55:53,554  WARN SimpleAsyncTaskExecutor-1 logger.aggregatedEvents:150 - StepExecution: id=2, version=1, name=step1, status=STARTED, exitStatus=EXECUTING, readCount=0, filterCount=0, writeCount=0 readSkipCount=0, writeSkipCount=0, processSkipCount=0, commitCount=0, rollbackCount=0, exitDescription=
09:55:53,561  WARN SimpleAsyncTaskExecutor-1 logger.aggregatedEvents:150 - XdChunkContextInfo [complete=false, stepExecution=StepExecution: id=2, version=1, name=step1, status=STARTED, exitStatus=EXECUTING, readCount=0, filterCount=0, writeCount=0 readSkipCount=0, writeSkipCount=0, processSkipCount=0, commitCount=0, rollbackCount=0, exitDescription=, attributes={}]
09:55:53,567  WARN SimpleAsyncTaskExecutor-1 logger.aggregatedEvents:150 - XdChunkContextInfo [complete=false, stepExecution=StepExecution: id=2, version=2, name=step1, status=STARTED, exitStatus=EXECUTING, readCount=0, filterCount=0, writeCount=0 readSkipCount=0, writeSkipCount=0, processSkipCount=0, commitCount=1, rollbackCount=0, exitDescription=, attributes={}]
09:55:53,573  WARN SimpleAsyncTaskExecutor-1 logger.aggregatedEvents:150 - StepExecution: id=2, version=2, name=step1, status=COMPLETED, exitStatus=COMPLETED, readCount=0, filterCount=0, writeCount=0 readSkipCount=0, writeSkipCount=0, processSkipCount=0, commitCount=1, rollbackCount=0, exitDescription=
09:55:53,580  WARN SimpleAsyncTaskExecutor-1 logger.aggregatedEvents:150 - JobExecution: id=2, version=1, startTime=Sat Apr 12 09:55:53 PDT 2014, endTime=Sat Apr 12 09:55:53 PDT 2014, lastUpdated=Sat Apr 12 09:55:53 PDT 2014, status=COMPLETED, exitStatus=exitCode=COMPLETED;exitDescription=, job=[JobInstance: id=2, version=0, Job=[myHttpJob]], jobParameters=[{random=0.07002785662707867}]
----

==== To receive job execution events

----
xd>job create --name myHttpJob --definition "httpJob" --deploy
xd>stream create --name jobExecutionEvents --definition "tap:job:myHttpJob.job >log" --deploy
xd>job launch myHttpJob
----

**Note:** The syntax for the tap that receives the job execution events is: `tap:job:<job-name>.job`

In the logging output of the container you should see something like the following when the job completes
----
10:06:41,579  WARN SimpleAsyncTaskExecutor-1 logger.jobExecutionEvents:150 - JobExecution: id=3, version=1, startTime=Sat Apr 12 10:06:41 PDT 2014, endTime=null, lastUpdated=Sat Apr 12 10:06:41 PDT 2014, status=STARTED, exitStatus=exitCode=UNKNOWN;exitDescription=, job=[JobInstance: id=3, version=0, Job=[myHttpJob]], jobParameters=[{random=0.3774227747555795}]
10:06:41,626  INFO SimpleAsyncTaskExecutor-1 support.SimpleJobLauncher:136 - Job: [FlowJob: [name=myHttpJob]] completed with the following parameters: [{random=0.3774227747555795}] and the following status: [COMPLETED]
10:06:41,626  WARN SimpleAsyncTaskExecutor-1 logger.jobExecutionEvents:150 - JobExecution: id=3, version=1, startTime=Sat Apr 12 10:06:41 PDT 2014, endTime=Sat Apr 12 10:06:41 PDT 2014, lastUpdated=Sat Apr 12 10:06:41 PDT 2014, status=COMPLETED, exitStatus=exitCode=COMPLETED;exitDescription=, job=[JobInstance: id=3, version=0, Job=[myHttpJob]], jobParameters=[{random=0.3774227747555795}]

----

==== To receive step execution events

----
xd>job create --name myHttpJob --definition "httpJob" --deploy
xd>stream create --name stepExecutionEvents --definition "tap:job:myHttpJob.step >log" --deploy
xd>job launch myHttpJob
----

**Note:** The syntax for the tap that receives the step execution events is: `tap:job:<job-name>.step`

In the logging output of the container you should see something like the following when the job completes
----

10:13:16,072  WARN SimpleAsyncTaskExecutor-1 logger.stepExecutionEvents:150 - StepExecution: id=6, version=1, name=step1, status=STARTED, exitStatus=EXECUTING, readCount=0, filterCount=0, writeCount=0 readSkipCount=0, writeSkipCount=0, processSkipCount=0, commitCount=0, rollbackCount=0, exitDescription=
10:13:16,092  WARN SimpleAsyncTaskExecutor-1 logger.stepExecutionEvents:150 - StepExecution: id=6, version=2, name=step1, status=COMPLETED, exitStatus=COMPLETED, readCount=0, filterCount=0, writeCount=0 readSkipCount=0, writeSkipCount=0, processSkipCount=0, commitCount=1, rollbackCount=0, exitDescription=

----

==== To receive item, skip and chunk events

----
xd>job create --name myHttpJob --definition "httpJob" --deploy

xd>stream create --name itemEvents --definition "tap:job:myHttpJob.item >log" --deploy
xd>stream create --name skipEvents --definition "tap:job:myHttpJob.skip >log" --deploy
xd>stream create --name chunkEvents --definition "tap:job:myHttpJob.chunk >log" --deploy

xd>job launch myHttpJob

----

**Note:** The syntax for the tap that receives the item events: `tap:job:<job-name>.item`,for skip events: `tap:job:<job-name>.skip` and for chunk events: `tap:job:<job-name>.chunk`

==== To disable the default listeners

----
xd>job create --name myHttpJob --definition "httpJob --listeners=disable" --deploy
----

==== To select specific listeners

To select specific listeners, specify comma separated list in `--listeners` option.
Following example illustrates the selection of job and step execution listeners only:

----
xd>job create --name myHttpJob --definition "httpJob --listeners=job,step" --deploy

----
**Note:**
List of options are: job, step, item, chunk and skip
The aggregated channel is registered if at least one of these default listeners are enabled.

For a complete example, please see the https://github.com/spring-projects/spring-xd-samples/tree/master/batch-notifications[Batch Notifications Sample] which is part of the https://github.com/spring-projects/spring-xd-samples[Spring XD Samples] repository.

=== Removing Batch Jobs

Batch Jobs can be deleted by executing:

----
xd:> job destroy helloSpringXD
----

Alternatively, one can just undeploy the job, keeping its definition for a future redeployment:

----
xd:> job undeploy helloSpringXD
----
